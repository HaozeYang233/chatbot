{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot_English",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbohTOgnoA8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "import re\n",
        "import time\n",
        "import collections\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piqZJ3MhALgA",
        "colab_type": "text"
      },
      "source": [
        "# **Data Processing：**\n",
        "\n",
        "1. Read the data in the 'conversation' list\n",
        "2. Use regular expressions to preprocess the data: first expand the English abbreviation, then remove unnecessary characters, only retain English characters and necessary punctuation\n",
        "3. Only keep sentences of 1 to 50 words in length\n",
        "4. Divide the data into two lists, question and answer\n",
        "5. 85% of the data is used for training and 15% of the data is used for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqYztalJpnPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readCorpus(path):\n",
        "    fp = open(path, \"r\", encoding='gb18030', errors='ignore')\n",
        "    content = fp.read().split('\\n')\n",
        "    fp.close()\n",
        "    return content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWKzK2z2popJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data cleansing and preprocessing with regular expressions\n",
        "def clean_text(input_text):\n",
        "    num = len(input_text)\n",
        "    for i in range(num):\n",
        "        input_text[i] = input_text[i].lower() # lower case\n",
        "        input_text[i] = re.sub(r\"I'm\", \"I am\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"i'm\", \"i am\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"im\", \"i am\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"Im\", \"I am\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"he's\", \"he is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"He's\", \"He is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"she's\", \"she is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"She's\", \"She is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"it's\", \"it is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"It's\", \"It is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"that's\", \"that is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"That's\", \"That is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"what's\", \"what is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"What's\", \"What is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"where's\", \"where is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"Where's\", \"Where is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"how's\", \"how is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"How's\", \"How is\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"bf\", \"boyfriend\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"gf\", \"girlfriend\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"\\,\", \" ,\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"\\.\", \" .\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"\\?\", \" ?\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"\\'ll\", \" will\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"\\'ve\", \" have\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"\\'re\", \" are\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"\\'d\", \" would\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"\\'re\", \" are\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"won't\", \"will not\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"can't\", \"cannot\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"n't\", \" not\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"n'\", \"ng\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"'bout\", \"about\", input_text[i])\n",
        "        input_text[i] = re.sub(r\"'til\", \"until\", input_text[i])\n",
        "        text_new = re.compile(u'[^0-9^a-z^A-Z^ ^,^.^?^]+', re.UNICODE)\n",
        "        input_text[i] = text_new.sub('', input_text[i])\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6s61q_0pumH",
        "colab_type": "code",
        "outputId": "bc074d5f-0cc0-4f18-e1eb-7714b5e31167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Read corpus\n",
        "conversation = readCorpus('conversationEnglish.txt')\n",
        "# Clean database\n",
        "clean_text(conversation)\n",
        "print(len(conversation))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux1pIvUsp1Ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Order the sentences into questions and answers\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for i in range(len(conversation) - 1):\n",
        "    questions.append(conversation[i])\n",
        "    answers.append(conversation[i+1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42RqwQaPp5ER",
        "colab_type": "code",
        "outputId": "38468c35-cc4b-460b-f5aa-4f20c9f492dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "#Output top five conversations\n",
        "for i in range(0, 5):\n",
        "    print(questions[i])\n",
        "    print(answers[i])\n",
        "    print(\"---------\")\n",
        "    \n",
        "print(len(questions))\n",
        "print(len(answers))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello , what is your name ?\n",
            "my name is sunday .\n",
            "---------\n",
            "my name is sunday .\n",
            "who designed you ?\n",
            "---------\n",
            "who designed you ?\n",
            "the most handsome programmer in the world .\n",
            "---------\n",
            "the most handsome programmer in the world .\n",
            "what is your name ?\n",
            "---------\n",
            "what is your name ?\n",
            "my name is sunday\n",
            "---------\n",
            "5018\n",
            "5018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWK9rVqbq2XB",
        "colab_type": "code",
        "outputId": "61991b4b-0b04-4084-f3d4-298580278560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "## Choose the sentences that the length is from 1～55, \n",
        "## And split the database into question and answer list.\n",
        "minLength = 1\n",
        "maxLength = 55\n",
        "questions_temp = []\n",
        "answers_temp = []\n",
        "select_questions = []\n",
        "select_answers = []\n",
        "\n",
        "i = 0\n",
        "for question in questions:\n",
        "    if len(question.split()) >= minLength and len(question.split()) <= maxLength:\n",
        "        questions_temp.append(question)\n",
        "        answers_temp.append(answers[i])\n",
        "    i += 1\n",
        "\n",
        "i = 0\n",
        "for answer in answers_temp:\n",
        "    if len(answer.split()) >= minLength and len(answer.split()) <= maxLength:\n",
        "        select_answers.append(answer)\n",
        "        select_questions.append(questions_temp[i])\n",
        "    i += 1\n",
        "    \n",
        "l_questions = len(questions) # length of question list\n",
        "l_answers = len(answers) #length of answer list\n",
        "divided = int(0.85*l_questions) #85% for training ，15% for testing\n",
        "    \n",
        "question_test = select_questions[divided:l_questions]\n",
        "answer_test = select_answers[divided:l_answers]\n",
        "question_train = select_questions[:divided]\n",
        "answer_train = select_answers[:divided]\n",
        "\n",
        "print(len(question_test))\n",
        "print(len(answer_test))\n",
        "print(len(question_train))\n",
        "print(len(answer_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "753\n",
            "753\n",
            "4265\n",
            "4265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5L-01LFBZ-n",
        "colab_type": "text"
      },
      "source": [
        "# **Data vectorization and dictionary establishment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoipkXfZhAiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependent on the frequency of each word.\n",
        "def build_dictionary(words, n_words, atleast=1):\n",
        "    count = [['PAD', 0], ['GO', 1], ['EOS', 2], ['UNK', 3]]\n",
        "    # Count the number of each word and sort it.\n",
        "    counter = collections.Counter(words).most_common(n_words) # map: [('words', number), (...), ...]\n",
        "    # Filter：delete the word using the counter\n",
        "    counter = [i for i in counter if i[1] >= atleast] # 'atleast' is the minimum number that the word have to appear\n",
        "    count.extend(counter) # make the counter as a list\n",
        "    dictionary = dict() # use dict() function to create a dictionary : dict(([key,value],[key,value]))\n",
        "    for word, _ in count:\n",
        "        dictionary[word] = len(dictionary)\n",
        "    data = list()\n",
        "    unk_count = 0\n",
        "    for word in words:\n",
        "        index = dictionary.get(word, 0)\n",
        "        if index == 0:\n",
        "            unk_count += 1\n",
        "        data.append(index)\n",
        "    count[0][1] = unk_count\n",
        "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    return data, count, dictionary, reversed_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i6PIPLqoA8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For question list\n",
        "concat_ques = ' '.join(question_train+question_test).split() # split sentence with each word\n",
        "# set select the differenct word, list can transfer it as a list\n",
        "vocabulary_size_ques = len(list(set(concat_ques))) # total number of vocabulary\n",
        "data_ques, count_ques, dictionary_ques, rev_dictionary_ques = build_dictionary(concat_ques, vocabulary_size_ques)\n",
        "\n",
        "## For answer list\n",
        "concat_ans = ' '.join(answer_train+answer_test).split()\n",
        "vocabulary_size_ans = len(list(set(concat_ans)))\n",
        "data_ans, count_ans, dictionary_ans, rev_dictionary_ans = build_dictionary(concat_ans, vocabulary_size_ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obP5dtS5oA8j",
        "colab_type": "code",
        "outputId": "fe86e163-5dd9-49fe-ee3b-b6cb1403d6d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "print('Question list:')\n",
        "print('Total number of vocabulary: %d'%(vocabulary_size_ques))\n",
        "print('Most common words', count_ques[4:10]) # print the data from 4th to 10th\n",
        "print('Size of dictionary:',len(dictionary_ques))\n",
        "print(\"Percentage that vocabulary used: {}%\".format(round(len(dictionary_ques)/vocabulary_size_ques,4)*100))\n",
        "\n",
        "print('Answer list:')\n",
        "print('Total number of vocabulary: %d'%(vocabulary_size_ans))\n",
        "print('Most common words', count_ans[4:10])\n",
        "print('Size of dictionary:',len(dictionary_ans))\n",
        "print(\"Percentage that vocabulary used: {}%\".format(round(len(dictionary_ans)/vocabulary_size_ans,4)*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question list:\n",
            "Total number of vocabulary: 2523\n",
            "Most common words [('.', 2238), (',', 2072), ('you', 1721), ('i', 1683), ('?', 955), ('to', 902)]\n",
            "Size of dictionary: 2527\n",
            "Percentage that vocabulary used: 100.16000000000001%\n",
            "Answer list:\n",
            "Total number of vocabulary: 2523\n",
            "Most common words [('.', 2238), (',', 2071), ('you', 1721), ('i', 1683), ('?', 954), ('to', 902)]\n",
            "Size of dictionary: 2527\n",
            "Percentage that vocabulary used: 100.16000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrFaNbYJ0AYL",
        "colab_type": "code",
        "outputId": "7f5f7767-b4f7-4459-e081-8d4774ec1ee8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Visualize word dictionary\n",
        "for key,value in dictionary_ques.items():\n",
        "    print('{key}\\t{value}'.format(key = key, value = value))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PAD\t0\n",
            "GO\t1\n",
            "EOS\t2\n",
            "UNK\t3\n",
            ".\t4\n",
            ",\t5\n",
            "you\t6\n",
            "i\t7\n",
            "?\t8\n",
            "to\t9\n",
            "is\t10\n",
            "a\t11\n",
            "the\t12\n",
            "are\t13\n",
            "am\t14\n",
            "do\t15\n",
            "not\t16\n",
            "what\t17\n",
            "it\t18\n",
            "have\t19\n",
            "bye\t20\n",
            "ok\t21\n",
            "and\t22\n",
            "of\t23\n",
            "my\t24\n",
            "that\t25\n",
            "me\t26\n",
            "will\t27\n",
            "in\t28\n",
            "your\t29\n",
            "can\t30\n",
            "conversation\t31\n",
            "end\t32\n",
            "for\t33\n",
            "no\t34\n",
            "go\t35\n",
            "then\t36\n",
            "like\t37\n",
            "eat\t38\n",
            "how\t39\n",
            "be\t40\n",
            "good\t41\n",
            "yes\t42\n",
            "very\t43\n",
            "so\t44\n",
            "well\t45\n",
            "hello\t46\n",
            "haha\t47\n",
            "see\t48\n",
            "hi\t49\n",
            "want\t50\n",
            "happy\t51\n",
            "come\t52\n",
            "thank\t53\n",
            "when\t54\n",
            "at\t55\n",
            "back\t56\n",
            "on\t57\n",
            "who\t58\n",
            "get\t59\n",
            "going\t60\n",
            "an\t61\n",
            "as\t62\n",
            "later\t63\n",
            "about\t64\n",
            "tell\t65\n",
            "feel\t66\n",
            "love\t67\n",
            "or\t68\n",
            "ti\t69\n",
            "know\t70\n",
            "but\t71\n",
            "too\t72\n",
            "we\t73\n",
            "this\t74\n",
            "ame\t75\n",
            "robot\t76\n",
            "class\t77\n",
            "there\t78\n",
            "with\t79\n",
            "its\t80\n",
            "all\t81\n",
            "welcome\t82\n",
            "hahaha\t83\n",
            "if\t84\n",
            "name\t85\n",
            "okay\t86\n",
            "buy\t87\n",
            "think\t88\n",
            "handsome\t89\n",
            "sunday\t90\n",
            "cannot\t91\n",
            "computer\t92\n",
            "did\t93\n",
            "should\t94\n",
            "just\t95\n",
            "really\t96\n",
            "why\t97\n",
            "now\t98\n",
            "chat\t99\n",
            "take\t100\n",
            "after\t101\n",
            "he\t102\n",
            "lets\t103\n",
            "more\t104\n",
            "also\t105\n",
            "wait\t106\n",
            "only\t107\n",
            "drink\t108\n",
            "much\t109\n",
            "together\t110\n",
            "today\t111\n",
            "joke\t112\n",
            "doing\t113\n",
            "software\t114\n",
            "artificial\t115\n",
            "work\t116\n",
            "old\t117\n",
            "still\t118\n",
            "things\t119\n",
            "make\t120\n",
            "than\t121\n",
            "tomorrow\t122\n",
            "playing\t123\n",
            "been\t124\n",
            "one\t125\n",
            "up\t126\n",
            "from\t127\n",
            "any\t128\n",
            "does\t129\n",
            "better\t130\n",
            "day\t131\n",
            "something\t132\n",
            "hard\t133\n",
            "which\t134\n",
            "boy\t135\n",
            "has\t136\n",
            "die\t137\n",
            "bad\t138\n",
            "yet\t139\n",
            "ask\t140\n",
            "cross\t141\n",
            "say\t142\n",
            "dont\t143\n",
            "stock\t144\n",
            "sad\t145\n",
            "said\t146\n",
            "study\t147\n",
            "nothing\t148\n",
            "they\t149\n",
            "need\t150\n",
            "night\t151\n",
            "right\t152\n",
            "where\t153\n",
            "was\t154\n",
            "intelligence\t155\n",
            "years\t156\n",
            "next\t157\n",
            "lot\t158\n",
            "homework\t159\n",
            "by\t160\n",
            "finished\t161\n",
            "never\t162\n",
            "would\t163\n",
            "home\t164\n",
            "body\t165\n",
            "human\t166\n",
            "market\t167\n",
            "try\t168\n",
            "favorite\t169\n",
            "money\t170\n",
            "eaten\t171\n",
            "find\t172\n",
            "some\t173\n",
            "afternoon\t174\n",
            "first\t175\n",
            "fine\t176\n",
            "problem\t177\n",
            "people\t178\n",
            "angry\t179\n",
            "could\t180\n",
            "hear\t181\n",
            "makes\t182\n",
            "bot\t183\n",
            "emotion\t184\n",
            "new\t185\n",
            "write\t186\n",
            "games\t187\n",
            "feeling\t188\n",
            "may\t189\n",
            "man\t190\n",
            "other\t191\n",
            "most\t192\n",
            "city\t193\n",
            "oh\t194\n",
            "always\t195\n",
            "talk\t196\n",
            "food\t197\n",
            "25\t198\n",
            "hey\t199\n",
            "python\t200\n",
            "robots\t201\n",
            "emotions\t202\n",
            "use\t203\n",
            "goodbye\t204\n",
            "soon\t205\n",
            "waiting\t206\n",
            "building\t207\n",
            "language\t208\n",
            "girl\t209\n",
            "boyfriend\t210\n",
            "mood\t211\n",
            "ammortal\t212\n",
            "sense\t213\n",
            "because\t214\n",
            "mad\t215\n",
            "little\t216\n",
            "myself\t217\n",
            "looking\t218\n",
            "year\t219\n",
            "learn\t220\n",
            "went\t221\n",
            "programmer\t222\n",
            "wow\t223\n",
            "great\t224\n",
            "writing\t225\n",
            "computers\t226\n",
            "sorry\t227\n",
            "many\t228\n",
            "help\t229\n",
            "let\t230\n",
            "already\t231\n",
            "teacher\t232\n",
            "male\t233\n",
            "happened\t234\n",
            "person\t235\n",
            "them\t236\n",
            "paper\t237\n",
            "eating\t238\n",
            "hungry\t239\n",
            "out\t240\n",
            "yourself\t241\n",
            "excellent\t242\n",
            "course\t243\n",
            "two\t244\n",
            "early\t245\n",
            "working\t246\n",
            "matter\t247\n",
            "anything\t248\n",
            "movies\t249\n",
            "every\t250\n",
            "understand\t251\n",
            "delicious\t252\n",
            "experience\t253\n",
            "supermarket\t254\n",
            "girlfriend\t255\n",
            "best\t256\n",
            "game\t257\n",
            "friends\t258\n",
            "own\t259\n",
            "si\t260\n",
            "sure\t261\n",
            "ames\t262\n",
            "project\t263\n",
            "look\t264\n",
            "lab\t265\n",
            "world\t266\n",
            "china\t267\n",
            "province\t268\n",
            "beautiful\t269\n",
            "break\t270\n",
            "alcohol\t271\n",
            "play\t272\n",
            "data\t273\n",
            "super\t274\n",
            "made\t275\n",
            "live\t276\n",
            "question\t277\n",
            "hal\t278\n",
            "learning\t279\n",
            "away\t280\n",
            "sleepy\t281\n",
            "cook\t282\n",
            "designed\t283\n",
            "hobby\t284\n",
            "female\t285\n",
            "hope\t286\n",
            "forbidden\t287\n",
            "lie\t288\n",
            "actually\t289\n",
            "program\t290\n",
            "maybe\t291\n",
            "here\t292\n",
            "morning\t293\n",
            "recently\t294\n",
            "brother\t295\n",
            "again\t296\n",
            "way\t297\n",
            "fast\t298\n",
            "self\t299\n",
            "sound\t300\n",
            "high\t301\n",
            "mind\t302\n",
            "enough\t303\n",
            "kind\t304\n",
            "used\t305\n",
            "electricity\t306\n",
            "same\t307\n",
            "crazy\t308\n",
            "feelings\t309\n",
            "big\t310\n",
            "anger\t311\n",
            "difficult\t312\n",
            "hate\t313\n",
            "someti\t314\n",
            "give\t315\n",
            "meal\t316\n",
            "school\t317\n",
            "while\t318\n",
            "she\t319\n",
            "programmed\t320\n",
            "capable\t321\n",
            "gossip\t322\n",
            "shaanxi\t323\n",
            "type\t324\n",
            "music\t325\n",
            "gender\t326\n",
            "nice\t327\n",
            "over\t328\n",
            "built\t329\n",
            "bots\t330\n",
            "easy\t331\n",
            "movie\t332\n",
            "worry\t333\n",
            "yesterday\t334\n",
            "start\t335\n",
            "ready\t336\n",
            "didnt\t337\n",
            "lost\t338\n",
            "read\t339\n",
            "finish\t340\n",
            "quite\t341\n",
            "tonight\t342\n",
            "umbrella\t343\n",
            "yulin\t344\n",
            "owner\t345\n",
            "birthday\t346\n",
            "friend\t347\n",
            "move\t348\n",
            "operating\t349\n",
            "hardware\t350\n",
            "watching\t351\n",
            "life\t352\n",
            "others\t353\n",
            "common\t354\n",
            "rest\t355\n",
            "late\t356\n",
            "appointment\t357\n",
            "email\t358\n",
            "last\t359\n",
            "forget\t360\n",
            "oclock\t361\n",
            "coming\t362\n",
            "house\t363\n",
            "ate\t364\n",
            "express\t365\n",
            "idol\t366\n",
            "amazing\t367\n",
            "young\t368\n",
            "missing\t369\n",
            "forever\t370\n",
            "another\t371\n",
            "call\t372\n",
            "fear\t373\n",
            "seen\t374\n",
            "earn\t375\n",
            "written\t376\n",
            "dinner\t377\n",
            "down\t378\n",
            "door\t379\n",
            "16\t380\n",
            "evening\t381\n",
            "key\t382\n",
            "noon\t383\n",
            "long\t384\n",
            "ever\t385\n",
            "certainly\t386\n",
            "business\t387\n",
            "heard\t388\n",
            "systems\t389\n",
            "mac\t390\n",
            "true\t391\n",
            "number\t392\n",
            "iron\t393\n",
            "everything\t394\n",
            "idea\t395\n",
            "online\t396\n",
            "needs\t397\n",
            "meet\t398\n",
            "done\t399\n",
            "half\t400\n",
            "probably\t401\n",
            "were\t402\n",
            "got\t403\n",
            "seems\t404\n",
            "open\t405\n",
            "pick\t406\n",
            "bring\t407\n",
            "far\t408\n",
            "gossips\t409\n",
            "introduce\t410\n",
            "introduction\t411\n",
            "brain\t412\n",
            "free\t413\n",
            "once\t414\n",
            "humans\t415\n",
            "talking\t416\n",
            "miss\t417\n",
            "shoes\t418\n",
            "unix\t419\n",
            "control\t420\n",
            "perfect\t421\n",
            "guess\t422\n",
            "might\t423\n",
            "often\t424\n",
            "sleep\t425\n",
            "jealous\t426\n",
            "shame\t427\n",
            "wish\t428\n",
            "busy\t429\n",
            "started\t430\n",
            "else\t431\n",
            "being\t432\n",
            "company\t433\n",
            "code\t434\n",
            "almost\t435\n",
            "change\t436\n",
            "raining\t437\n",
            "rain\t438\n",
            "station\t439\n",
            "watch\t440\n",
            "smart\t441\n",
            "played\t442\n",
            "poker\t443\n",
            "addicted\t444\n",
            "science\t445\n",
            "personality\t446\n",
            "laugh\t447\n",
            "allowed\t448\n",
            "such\t449\n",
            "system\t450\n",
            "windows\t451\n",
            "numbers\t452\n",
            "pretty\t453\n",
            "complex\t454\n",
            "amportant\t455\n",
            "these\t456\n",
            "someone\t457\n",
            "mean\t458\n",
            "depends\t459\n",
            "anyone\t460\n",
            "dream\t461\n",
            "days\t462\n",
            "saw\t463\n",
            "bank\t464\n",
            "pay\t465\n",
            "came\t466\n",
            "few\t467\n",
            "cant\t468\n",
            "able\t469\n",
            "anyway\t470\n",
            "pass\t471\n",
            "nervous\t472\n",
            "off\t473\n",
            "thing\t474\n",
            "directly\t475\n",
            "outside\t476\n",
            "sent\t477\n",
            "room\t478\n",
            "shopping\t479\n",
            "concern\t480\n",
            "fruit\t481\n",
            "job\t482\n",
            "pop\t483\n",
            "boys\t484\n",
            "age\t485\n",
            "lady\t486\n",
            "gentlemen\t487\n",
            "thinking\t488\n",
            "meaning\t489\n",
            "cloning\t490\n",
            "machine\t491\n",
            "death\t492\n",
            "chatterbox\t493\n",
            "programming\t494\n",
            "wear\t495\n",
            "our\t496\n",
            "without\t497\n",
            "linux\t498\n",
            "os\t499\n",
            "ai\t500\n",
            "plan\t501\n",
            "personal\t502\n",
            "interested\t503\n",
            "character\t504\n",
            "father\t505\n",
            "cake\t506\n",
            "even\t507\n",
            "before\t508\n",
            "believe\t509\n",
            "sadness\t510\n",
            "interesting\t511\n",
            "pain\t512\n",
            "alone\t513\n",
            "ample\t514\n",
            "rich\t515\n",
            "tired\t516\n",
            "power\t517\n",
            "into\t518\n",
            "low\t519\n",
            "dollar\t520\n",
            "called\t521\n",
            "economics\t522\n",
            "under\t523\n",
            "thanks\t524\n",
            "meeting\t525\n",
            "send\t526\n",
            "run\t527\n",
            "exam\t528\n",
            "pressure\t529\n",
            "classmates\t530\n",
            "shanghai\t531\n",
            "ah\t532\n",
            "his\t533\n",
            "forgot\t534\n",
            "cooking\t535\n",
            "water\t536\n",
            "minutes\t537\n",
            "fun\t538\n",
            "remember\t539\n",
            "small\t540\n",
            "um\t541\n",
            "put\t542\n",
            "chicken\t543\n",
            "hot\t544\n",
            "large\t545\n",
            "full\t546\n",
            "making\t547\n",
            "guns\t548\n",
            "woman\t549\n",
            "sing\t550\n",
            "songs\t551\n",
            "lin\t552\n",
            "jj\t553\n",
            "comedy\t554\n",
            "comfort\t555\n",
            "form\t556\n",
            "until\t557\n",
            "create\t558\n",
            "real\t559\n",
            "lack\t560\n",
            "dreams\t561\n",
            "especially\t562\n",
            "including\t563\n",
            "variety\t564\n",
            "ibm\t565\n",
            "happen\t566\n",
            "walk\t567\n",
            "series\t568\n",
            "kinds\t569\n",
            "picky\t570\n",
            "eaters\t571\n",
            "everywhere\t572\n",
            "brothers\t573\n",
            "news\t574\n",
            "gone\t575\n",
            "complicated\t576\n",
            "face\t577\n",
            "unless\t578\n",
            "although\t579\n",
            "definitely\t580\n",
            "around\t581\n",
            "usually\t582\n",
            "possible\t583\n",
            "happiness\t584\n",
            "afraid\t585\n",
            "bored\t586\n",
            "embarrassed\t587\n",
            "rate\t588\n",
            "exchange\t589\n",
            "special\t590\n",
            "powerful\t591\n",
            "greetings\t592\n",
            "auntie\t593\n",
            "check\t594\n",
            "im\t595\n",
            "hand\t596\n",
            "set\t597\n",
            "okok\t598\n",
            "asked\t599\n",
            "week\t600\n",
            "hour\t601\n",
            "bit\t602\n",
            "care\t603\n",
            "topic\t604\n",
            "road\t605\n",
            "experi\t606\n",
            "esti\t607\n",
            "says\t608\n",
            "yeah\t609\n",
            "juice\t610\n",
            "coffee\t611\n",
            "poor\t612\n",
            "continue\t613\n",
            "accompany\t614\n",
            "bus\t615\n",
            "stop\t616\n",
            "arrived\t617\n",
            "snacks\t618\n",
            "roommate\t619\n",
            "result\t620\n",
            "restaurant\t621\n",
            "must\t622\n",
            "everyone\t623\n",
            "wants\t624\n",
            "wrong\t625\n",
            "reference\t626\n",
            "pot\t627\n",
            "xian\t628\n",
            "title\t629\n",
            "anybody\t630\n",
            "cat\t631\n",
            "etc\t632\n",
            "found\t633\n",
            "liverpool\t634\n",
            "library\t635\n",
            "supper\t636\n",
            "cool\t637\n",
            "3\t638\n",
            "cry\t639\n",
            "machines\t640\n",
            "entity\t641\n",
            "hypothetical\t642\n",
            "clone\t643\n",
            "digital\t644\n",
            "bend\t645\n",
            "stupid\t646\n",
            "leaving\t647\n",
            "least\t648\n",
            "existence\t649\n",
            "size\t650\n",
            "pure\t651\n",
            "logic\t652\n",
            "runs\t653\n",
            "running\t654\n",
            "mate\t655\n",
            "breathe\t656\n",
            "meat\t657\n",
            "purposes\t658\n",
            "subjects\t659\n",
            "natural\t660\n",
            "processing\t661\n",
            "marvel\t662\n",
            "mother\t663\n",
            "asking\t664\n",
            "please\t665\n",
            "obscure\t666\n",
            "explain\t667\n",
            "agree\t668\n",
            "java\t669\n",
            "means\t670\n",
            "between\t671\n",
            "understanding\t672\n",
            "bragging\t673\n",
            "unhappy\t674\n",
            "scared\t675\n",
            "become\t676\n",
            "ashamed\t677\n",
            "ability\t678\n",
            "energy\t679\n",
            "several\t680\n",
            "salary\t681\n",
            "sell\t682\n",
            "trading\t683\n",
            "unit\t684\n",
            "investment\t685\n",
            "production\t686\n",
            "distribution\t687\n",
            "resources\t688\n",
            "bought\t689\n",
            ",000\t690\n",
            "1\t691\n",
            "health\t692\n",
            "cold\t693\n",
            "trouble\t694\n",
            "reading\t695\n",
            "saying\t696\n",
            "hahahaha\t697\n",
            "david\t698\n",
            "monday\t699\n",
            "github\t700\n",
            "met\t701\n",
            "line\t702\n",
            "wanted\t703\n",
            "graduate\t704\n",
            "earlier\t705\n",
            "medicine\t706\n",
            "studying\t707\n",
            "contact\t708\n",
            "thought\t709\n",
            "had\t710\n",
            "changed\t711\n",
            "16th\t712\n",
            "floor\t713\n",
            "classmate\t714\n",
            "using\t715\n",
            "given\t716\n",
            "downstairs\t717\n",
            "her\t718\n",
            "relax\t719\n",
            "cream\t720\n",
            "wash\t721\n",
            "cancel\t722\n",
            "fact\t723\n",
            "guilty\t724\n",
            "classroom\t725\n",
            "seriously\t726\n",
            "process\t727\n",
            "candy\t728\n",
            "southampton\t729\n",
            "5\t730\n",
            "left\t731\n",
            "future\t732\n",
            "ten\t733\n",
            "cut\t734\n",
            "vegetables\t735\n",
            "uncomfortable\t736\n",
            "close\t737\n",
            "dress\t738\n",
            "speed\t739\n",
            "center\t740\n",
            "weather\t741\n",
            "milk\t742\n",
            "sounds\t743\n",
            "wont\t744\n",
            "6\t745\n",
            "through\t746\n",
            "took\t747\n",
            "sort\t748\n",
            "though\t749\n",
            "sapient\t750\n",
            "count\t751\n",
            "incapable\t752\n",
            "device\t753\n",
            "instructions\t754\n",
            "calculations\t755\n",
            "context\t756\n",
            "amply\t757\n",
            "smith\t758\n",
            "bug\t759\n",
            "lying\t760\n",
            "ammature\t761\n",
            "spouse\t762\n",
            "november\t763\n",
            "1994\t764\n",
            "18\t765\n",
            "horoscope\t766\n",
            "scorpio\t767\n",
            "iq\t768\n",
            "peace\t769\n",
            "branch\t770\n",
            "engineering\t771\n",
            "inspired\t772\n",
            "contrary\t773\n",
            "fighting\t774\n",
            "terminated\t775\n",
            "born\t776\n",
            "attempts\t777\n",
            "amulate\t778\n",
            "eliza\t779\n",
            "programs\t780\n",
            "fool\t781\n",
            "original\t782\n",
            "eventually\t783\n",
            "creating\t784\n",
            "except\t785\n",
            "creativity\t786\n",
            "ambition\t787\n",
            "subjectivity\t788\n",
            "amagine\t789\n",
            "supports\t790\n",
            "anywhere\t791\n",
            "save\t792\n",
            "includes\t793\n",
            "legs\t794\n",
            "method\t795\n",
            "metal\t796\n",
            "allow\t797\n",
            "record\t798\n",
            "practical\t799\n",
            "product\t800\n",
            "desks\t801\n",
            "sales\t802\n",
            "entertainment\t803\n",
            "consume\t804\n",
            "location\t805\n",
            "clones\t806\n",
            "cup\t807\n",
            "blue\t808\n",
            "masked\t809\n",
            "secret\t810\n",
            "complexity\t811\n",
            "ambiguity\t812\n",
            "refuse\t813\n",
            "seem\t814\n",
            "familiar\t815\n",
            "ugly\t816\n",
            "obvious\t817\n",
            "amplementation\t818\n",
            "condition\t819\n",
            "reproduction\t820\n",
            "changes\t821\n",
            "definition\t822\n",
            "acceptance\t823\n",
            "arrogant\t824\n",
            "responsible\t825\n",
            "emotional\t826\n",
            "boring\t827\n",
            "relationship\t828\n",
            "beings\t829\n",
            "shortage\t830\n",
            "ament\t831\n",
            "spiderman\t832\n",
            "teknolust\t833\n",
            "book\t834\n",
            "version\t835\n",
            "issues\t836\n",
            "monster\t837\n",
            "cities\t838\n",
            "rings\t839\n",
            "recommend\t840\n",
            "central\t841\n",
            "conditions\t842\n",
            "top\t843\n",
            "aunt\t844\n",
            "attention\t845\n",
            "beijing\t846\n",
            "ive\t847\n",
            "choose\t848\n",
            "points\t849\n",
            "tutor\t850\n",
            "delay\t851\n",
            "havent\t852\n",
            "leave\t853\n",
            "prepare\t854\n",
            "sleeping\t855\n",
            "tight\t856\n",
            "card\t857\n",
            "return\t858\n",
            "four\t859\n",
            "exams\t860\n",
            "taken\t861\n",
            "spent\t862\n",
            "london\t863\n",
            "brief\t864\n",
            "second\t865\n",
            "show\t866\n",
            "visa\t867\n",
            "questions\t868\n",
            "previous\t869\n",
            "wednesday\t870\n",
            "ill\t871\n",
            "amated\t872\n",
            "living\t873\n",
            "keys\t874\n",
            "lunch\t875\n",
            "potatoes\t876\n",
            "dishes\t877\n",
            "rice\t878\n",
            "order\t879\n",
            "cheering\t880\n",
            "12\t881\n",
            "five\t882\n",
            "less\t883\n",
            "considered\t884\n",
            "pm\t885\n",
            "red\t886\n",
            "mainly\t887\n",
            "bedroom\t888\n",
            "ha\t889\n",
            "three\t890\n",
            "phone\t891\n",
            "accept\t892\n",
            "correct\t893\n",
            "heart\t894\n",
            "united\t895\n",
            "states\t896\n",
            "listen\t897\n",
            "cotton\t898\n",
            "snack\t899\n",
            "quickly\t900\n",
            "bother\t901\n",
            "thesis\t902\n",
            "stay\t903\n",
            "message\t904\n",
            "place\t905\n",
            "curious\t906\n",
            "noodles\t907\n",
            "functions\t908\n",
            "sentient\t909\n",
            "construct\t910\n",
            "perpetuated\t911\n",
            "indefinitely\t912\n",
            "training\t913\n",
            "robotics\t914\n",
            "effectively\t915\n",
            "cramped\t916\n",
            "binary\t917\n",
            "amprove\t918\n",
            "electronic\t919\n",
            "accuracy\t920\n",
            "capacity\t921\n",
            "invented\t922\n",
            "memory\t923\n",
            "circuit\t924\n",
            "those\t925\n",
            "dr\t926\n",
            "emote\t927\n",
            "jealousy\t928\n",
            "reason\t929\n",
            "piece\t930\n",
            "emulate\t931\n",
            "communicate\t932\n",
            "act\t933\n",
            "exactly\t934\n",
            "studied\t935\n",
            "relationships\t936\n",
            "understands\t937\n",
            "dog\t938\n",
            "rabbit\t939\n",
            "lemon\t940\n",
            "cow\t941\n",
            "government\t942\n",
            "anymore\t943\n",
            "dishonest\t944\n",
            "addict\t945\n",
            "cheating\t946\n",
            "paranoid\t947\n",
            "cute\t948\n",
            "prices\t949\n",
            "price\t950\n",
            "hangzhou\t951\n",
            "foods\t952\n",
            "onion\t953\n",
            "toowho\t954\n",
            "clever\t955\n",
            "dedicated\t956\n",
            "commanders\t957\n",
            "permanent\t958\n",
            "replication\t959\n",
            "laughs\t960\n",
            "theme\t961\n",
            "features\t962\n",
            "combat\t963\n",
            "broad\t964\n",
            "definitions\t965\n",
            "stretching\t966\n",
            "strengthening\t967\n",
            "anthropomorphic\t968\n",
            "machinery\t969\n",
            "r\t970\n",
            ".u\t971\n",
            ".r\t972\n",
            "rosengs\t973\n",
            "universal\t974\n",
            "karel\t975\n",
            "capers\t976\n",
            "routine\t977\n",
            "manual\t978\n",
            "wellknown\t979\n",
            "attempt\t980\n",
            "temporarily\t981\n",
            "chatter\t982\n",
            "speaks\t983\n",
            "tangible\t984\n",
            "desires\t985\n",
            "amal\t986\n",
            "works\t987\n",
            "effective\t988\n",
            "narrow\t989\n",
            "breeding\t990\n",
            "fail\t991\n",
            "track\t992\n",
            "borrow\t993\n",
            "sugar\t994\n",
            "books\t995\n",
            "color\t996\n",
            "observation\t997\n",
            "nature\t998\n",
            "alice\t999\n",
            "wonderland\t1000\n",
            "hat\t1001\n",
            "baking\t1002\n",
            "zen\t1003\n",
            "flat\t1004\n",
            "readability\t1005\n",
            "exception\t1006\n",
            "rules\t1007\n",
            "errors\t1008\n",
            "instead\t1009\n",
            "preferably\t1010\n",
            "describe\t1011\n",
            "namespaces\t1012\n",
            "c\t1013\n",
            "yolo\t1014\n",
            "define\t1015\n",
            "distinguishes\t1016\n",
            "organisms\t1017\n",
            "growth\t1018\n",
            "functional\t1019\n",
            "activity\t1020\n",
            "travel\t1021\n",
            "predictable\t1022\n",
            "trying\t1023\n",
            "worried\t1024\n",
            "highly\t1025\n",
            "offend\t1026\n",
            "chatting\t1027\n",
            "drunk\t1028\n",
            "amused\t1029\n",
            "processor\t1030\n",
            "requires\t1031\n",
            "paid\t1032\n",
            "structure\t1033\n",
            "agent\t1034\n",
            "solaris\t1035\n",
            "developed\t1036\n",
            "position\t1037\n",
            "algorithm\t1038\n",
            "safe\t1039\n",
            "fictional\t1040\n",
            "dead\t1041\n",
            "godzilla\t1042\n",
            "ring\t1043\n",
            "king\t1044\n",
            "invest\t1045\n",
            "casino\t1046\n",
            "predict\t1047\n",
            "lawyer\t1048\n",
            "skills\t1049\n",
            "mutual\t1050\n",
            "funds\t1051\n",
            "single\t1052\n",
            "beat\t1053\n",
            "actions\t1054\n",
            "currency\t1055\n",
            "medium\t1056\n",
            "value\t1057\n",
            "goods\t1058\n",
            "transactions\t1059\n",
            "wealth\t1060\n",
            "labor\t1061\n",
            "finance\t1062\n",
            "taxation\t1063\n",
            "related\t1064\n",
            "technically\t1065\n",
            "fill\t1066\n",
            "peoples\t1067\n",
            "nobody\t1068\n",
            "pays\t1069\n",
            "raise\t1070\n",
            "consumption\t1071\n",
            "pleasure\t1072\n",
            "resting\t1073\n",
            "letter\t1074\n",
            "resume\t1075\n",
            "chinese\t1076\n",
            "child\t1077\n",
            "casual\t1078\n",
            "draft\t1079\n",
            "seeing\t1080\n",
            "mail\t1081\n",
            "us\t1082\n",
            "demo\t1083\n",
            "indeed\t1084\n",
            "glance\t1085\n",
            "god\t1086\n",
            "share\t1087\n",
            "currently\t1088\n",
            "internship\t1089\n",
            "technology\t1090\n",
            "postponed\t1091\n",
            "students\t1092\n",
            "nope\t1093\n",
            "necessarily\t1094\n",
            "each\t1095\n",
            "friday\t1096\n",
            "stamp\t1097\n",
            "college\t1098\n",
            "chapter\t1099\n",
            "proof\t1100\n",
            "student\t1101\n",
            "office\t1102\n",
            "59\t1103\n",
            "mobile\t1104\n",
            "network\t1105\n",
            "calling\t1106\n",
            "abroad\t1107\n",
            "plane\t1108\n",
            "ppt\t1109\n",
            "4\t1110\n",
            "comfortable\t1111\n",
            "subject\t1112\n",
            "lecture\t1113\n",
            "2\t1114\n",
            "deadline\t1115\n",
            "delayed\t1116\n",
            "doesnt\t1117\n",
            "told\t1118\n",
            "weekend\t1119\n",
            "spend\t1120\n",
            "train\t1121\n",
            "beginning\t1122\n",
            "inside\t1123\n",
            "yep\t1124\n",
            "receive\t1125\n",
            "spare\t1126\n",
            "drank\t1127\n",
            "ice\t1128\n",
            "throw\t1129\n",
            "roommates\t1130\n",
            "pack\t1131\n",
            "dish\t1132\n",
            "shower\t1133\n",
            "amate\t1134\n",
            "appearance\t1135\n",
            "pair\t1136\n",
            "bag\t1137\n",
            "mistake\t1138\n",
            "eyes\t1139\n",
            "chewing\t1140\n",
            "cleaned\t1141\n",
            "washed\t1142\n",
            "oops\t1143\n",
            "progress\t1144\n",
            "teachers\t1145\n",
            "longer\t1146\n",
            "20\t1147\n",
            "cheer\t1148\n",
            "60\t1149\n",
            "hurry\t1150\n",
            "words\t1151\n",
            "envelope\t1152\n",
            "received\t1153\n",
            "healthy\t1154\n",
            "forgetting\t1155\n",
            "saturday\t1156\n",
            "breakfast\t1157\n",
            "classes\t1158\n",
            "obviously\t1159\n",
            "luck\t1160\n",
            "useful\t1161\n",
            "expensive\t1162\n",
            "finishing\t1163\n",
            "coat\t1164\n",
            "prepared\t1165\n",
            "reached\t1166\n",
            "bottom\t1167\n",
            "skirt\t1168\n",
            "sneakers\t1169\n",
            "cherries\t1170\n",
            "app\t1171\n",
            "copyright\t1172\n",
            "apple\t1173\n",
            "add\t1174\n",
            "forecast\t1175\n",
            "takes\t1176\n",
            "430\t1177\n",
            "lately\t1178\n",
            "generally\t1179\n",
            "burger\t1180\n",
            "remind\t1181\n",
            "stayed\t1182\n",
            "slept\t1183\n",
            "happily\t1184\n",
            "32\t1185\n",
            "clean\t1186\n",
            "part\t1187\n",
            "bowl\t1188\n",
            "rainproof\t1189\n",
            "exercise\t1190\n",
            "enjoy\t1191\n",
            "starving\t1192\n",
            "praised\t1193\n",
            "fortunately\t1194\n",
            "otherwise\t1195\n",
            "buying\t1196\n",
            "word\t1197\n",
            "fixed\t1198\n",
            "western\t1199\n",
            "arrive\t1200\n",
            "wine\t1201\n",
            "wales\t1202\n",
            "companies\t1203\n",
            "resumes\t1204\n",
            "feels\t1205\n",
            "test\t1206\n",
            "essay\t1207\n",
            "helped\t1208\n",
            "thats\t1209\n",
            "sky\t1210\n",
            "text\t1211\n",
            "heavy\t1212\n",
            "hold\t1213\n",
            "todays\t1214\n",
            "field\t1215\n",
            "sophisticated\t1216\n",
            "commander\t1217\n",
            "linguistic\t1218\n",
            "copied\t1219\n",
            "functionally\t1220\n",
            "speaking\t1221\n",
            "backed\t1222\n",
            "respond\t1223\n",
            "easily\t1224\n",
            "characteristics\t1225\n",
            "therefore\t1226\n",
            "processes\t1227\n",
            "killed\t1228\n",
            "database\t1229\n",
            "motormouth\t1230\n",
            "above\t1231\n",
            "math\t1232\n",
            "upload\t1233\n",
            "copy\t1234\n",
            "store\t1235\n",
            "plenty\t1236\n",
            "ram\t1237\n",
            "reside\t1238\n",
            "server\t1239\n",
            "sti\t1240\n",
            "lots\t1241\n",
            "interests\t1242\n",
            "fond\t1243\n",
            "digits\t1244\n",
            "galaxy\t1245\n",
            "internet\t1246\n",
            "engine\t1247\n",
            "performs\t1248\n",
            "operations\t1249\n",
            "perform\t1250\n",
            "regarded\t1251\n",
            "argue\t1252\n",
            "john\t1253\n",
            "loom\t1254\n",
            "amplements\t1255\n",
            "parts\t1256\n",
            "macos\t1257\n",
            "basic\t1258\n",
            "access\t1259\n",
            "hurt\t1260\n",
            "support\t1261\n",
            "names\t1262\n",
            "uses\t1263\n",
            "nasa\t1264\n",
            "carrying\t1265\n",
            "comes\t1266\n",
            "mask\t1267\n",
            "doubt\t1268\n",
            "temptation\t1269\n",
            "counts\t1270\n",
            "opinion\t1271\n",
            "hobbies\t1272\n",
            ".00\t1273\n",
            "husband\t1274\n",
            "photo\t1275\n",
            "unfortunately\t1276\n",
            "arrogance\t1277\n",
            "awesome\t1278\n",
            "fairly\t1279\n",
            "acting\t1280\n",
            "case\t1281\n",
            "environment\t1282\n",
            "incorrect\t1283\n",
            "somehow\t1284\n",
            "expressed\t1285\n",
            "current\t1286\n",
            "internal\t1287\n",
            "state\t1288\n",
            "source\t1289\n",
            "corrupt\t1290\n",
            "lag\t1291\n",
            "noticed\t1292\n",
            "party\t1293\n",
            "toward\t1294\n",
            "referred\t1295\n",
            "entities\t1296\n",
            "lonely\t1297\n",
            "experiencing\t1298\n",
            "supply\t1299\n",
            "tried\t1300\n",
            "guy\t1301\n",
            "speedrun\t1302\n",
            "ji\t1303\n",
            "ammy\t1304\n",
            "per\t1305\n",
            "buddhist\t1306\n",
            "shot\t1307\n",
            "weevils\t1308\n",
            "carolina\t1309\n",
            "became\t1310\n",
            "known\t1311\n",
            "kayak\t1312\n",
            "puns\t1313\n",
            "murderer\t1314\n",
            "frosted\t1315\n",
            "flakes\t1316\n",
            "killer\t1317\n",
            "automobile\t1318\n",
            "cheetah\t1319\n",
            "alien\t1320\n",
            "eggstraterrestrial\t1321\n",
            "assistant\t1322\n",
            "serious\t1323\n",
            "thief\t1324\n",
            "dance\t1325\n",
            "port\t1326\n",
            "canned\t1327\n",
            "jokes\t1328\n",
            "humor\t1329\n",
            "rates\t1330\n",
            "pieces\t1331\n",
            "problems\t1332\n",
            "charge\t1333\n",
            "month\t1334\n",
            "originally\t1335\n",
            "ampeached\t1336\n",
            "amendment\t1337\n",
            "governor\t1338\n",
            "ass\t1339\n",
            "cheat\t1340\n",
            "ways\t1341\n",
            "psycho\t1342\n",
            "sincere\t1343\n",
            "honest\t1344\n",
            "pedantic\t1345\n",
            "disgusting\t1346\n",
            "unattractive\t1347\n",
            "spending\t1348\n",
            "sunny\t1349\n",
            "opinions\t1350\n",
            "slick\t1351\n",
            "hide\t1352\n",
            "harder\t1353\n",
            "upset\t1354\n",
            "keep\t1355\n",
            "slowly\t1356\n",
            "sooner\t1357\n",
            "opti\t1358\n",
            "difficulty\t1359\n",
            "instructor\t1360\n",
            "annoying\t1361\n",
            "listening\t1362\n",
            "deal\t1363\n",
            "photos\t1364\n",
            "ryan\t1365\n",
            "knowledge\t1366\n",
            "cooked\t1367\n",
            "cola\t1368\n",
            "tea\t1369\n",
            "speech\t1370\n",
            "chili\t1371\n",
            "quarreling\t1372\n",
            "women\t1373\n",
            "men\t1374\n",
            "passed\t1375\n",
            "master\t1376\n",
            "tells\t1377\n",
            "represents\t1378\n",
            "suspect\t1379\n",
            "contradictory\t1380\n",
            "totally\t1381\n",
            "ampler\t1382\n",
            "guesswork\t1383\n",
            "beauty\t1384\n",
            "ming\t1385\n",
            "amplicity\t1386\n",
            "nesting\t1387\n",
            "interval\t1388\n",
            "compact\t1389\n",
            "utility\t1390\n",
            "nor\t1391\n",
            "violate\t1392\n",
            "tolerate\t1393\n",
            "multiple\t1394\n",
            "possibilities\t1395\n",
            "solution\t1396\n",
            "wonderful\t1397\n",
            "upsets\t1398\n",
            "inconsistencies\t1399\n",
            "lived\t1400\n",
            "minerals\t1401\n",
            "constant\t1402\n",
            "suggestion\t1403\n",
            "ahead\t1404\n",
            "seafood\t1405\n",
            "credit\t1406\n",
            "tolerance\t1407\n",
            "unique\t1408\n",
            "trust\t1409\n",
            "friendship\t1410\n",
            "pride\t1411\n",
            "helpless\t1412\n",
            "offended\t1413\n",
            "complain\t1414\n",
            "awake\t1415\n",
            "drinks\t1416\n",
            "nutrition\t1417\n",
            "experienced\t1418\n",
            "chatterbot\t1419\n",
            "compli\t1420\n",
            "yoda\t1421\n",
            "grammatical\t1422\n",
            "blade\t1423\n",
            "runner\t1424\n",
            "xfind\t1425\n",
            "released\t1426\n",
            "2002\t1427\n",
            "story\t1428\n",
            "comic\t1429\n",
            "scifi\t1430\n",
            "thrill\t1431\n",
            "zombie\t1432\n",
            "named\t1433\n",
            "ruby\t1434\n",
            "sun\t1435\n",
            "hal9000\t1436\n",
            "heuristic\t1437\n",
            "matrix\t1438\n",
            "harley\t1439\n",
            "alive\t1440\n",
            "solve\t1441\n",
            "threatens\t1442\n",
            "japanese\t1443\n",
            "york\t1444\n",
            "spider\t1445\n",
            "peter\t1446\n",
            "parker\t1447\n",
            "lord\t1448\n",
            "quveut\t1449\n",
            "scary\t1450\n",
            "logique\t1451\n",
            "heuristique\t1452\n",
            "algorithmique\t1453\n",
            "flaws\t1454\n",
            "famous\t1455\n",
            "2001\t1456\n",
            "financing\t1457\n",
            "hike\t1458\n",
            "taiwangs\t1459\n",
            "tool\t1460\n",
            "storage\t1461\n",
            "account\t1462\n",
            "commodity\t1463\n",
            "acts\t1464\n",
            "equivalent\t1465\n",
            "services\t1466\n",
            "largescale\t1467\n",
            "consumer\t1468\n",
            "scarce\t1469\n",
            "stocks\t1470\n",
            "popular\t1471\n",
            "collect\t1472\n",
            "luxury\t1473\n",
            "necessary\t1474\n",
            "capital\t1475\n",
            "yuan\t1476\n",
            "public\t1477\n",
            "offering\t1478\n",
            "shareholders\t1479\n",
            "kindly\t1480\n",
            "skys\t1481\n",
            "oki\t1482\n",
            "amproved\t1483\n",
            "convers\t1484\n",
            "fourth\t1485\n",
            "youth\t1486\n",
            "mothers\t1487\n",
            "surprise\t1488\n",
            "sister\t1489\n",
            "concord\t1490\n",
            "530\t1491\n",
            "kids\t1492\n",
            "uhhuh\t1493\n",
            "english\t1494\n",
            "adding\t1495\n",
            "build\t1496\n",
            "chose\t1497\n",
            "choice\t1498\n",
            "accounts\t1499\n",
            "40\t1500\n",
            "total\t1501\n",
            "score\t1502\n",
            "listens\t1503\n",
            "feedback\t1504\n",
            "sending\t1505\n",
            "april\t1506\n",
            "emailed\t1507\n",
            "fooled\t1508\n",
            "effect\t1509\n",
            "oneandahalf\t1510\n",
            "130\t1511\n",
            "dare\t1512\n",
            "struggling\t1513\n",
            "remembered\t1514\n",
            "returning\t1515\n",
            "insomnia\t1516\n",
            "bright\t1517\n",
            "vacation\t1518\n",
            "final\t1519\n",
            "1134\t1520\n",
            "twelve\t1521\n",
            "id\t1522\n",
            "jiangsu\t1523\n",
            "shixuan\t1524\n",
            "safely\t1525\n",
            "beneficial\t1526\n",
            "learned\t1527\n",
            "regretful\t1528\n",
            "stressful\t1529\n",
            "theres\t1530\n",
            "anxiety\t1531\n",
            "disorder\t1532\n",
            "depression\t1533\n",
            "recovering\t1534\n",
            "taking\t1535\n",
            "recover\t1536\n",
            "afterwards\t1537\n",
            "wang\t1538\n",
            "junwen\t1539\n",
            "everyones\t1540\n",
            "nights\t1541\n",
            "opportunity\t1542\n",
            "sign\t1543\n",
            "studies\t1544\n",
            "looked\t1545\n",
            "examiner\t1546\n",
            "supervisor\t1547\n",
            "consent\t1548\n",
            "report\t1549\n",
            "enrollment\t1550\n",
            "cards\t1551\n",
            "international\t1552\n",
            "roaming\t1553\n",
            "review\t1554\n",
            "main\t1555\n",
            "note\t1556\n",
            "senior\t1557\n",
            "cheered\t1558\n",
            "inquiring\t1559\n",
            "lounge\t1560\n",
            "lab3\t1561\n",
            "arrange\t1562\n",
            "assignments\t1563\n",
            "aments\t1564\n",
            "printing\t1565\n",
            "1pm\t1566\n",
            "dry\t1567\n",
            "pushed\t1568\n",
            "cheers\t1569\n",
            "delete\t1570\n",
            "principle\t1571\n",
            "repaint\t1572\n",
            "picture\t1573\n",
            "starts\t1574\n",
            "11\t1575\n",
            "writes\t1576\n",
            "sleeps\t1577\n",
            "during\t1578\n",
            "worth\t1579\n",
            "detailed\t1580\n",
            "court\t1581\n",
            "yard\t1582\n",
            "rn\t1583\n",
            "mins\t1584\n",
            "sec\t1585\n",
            "hfs\t1586\n",
            "sorted\t1587\n",
            "reply\t1588\n",
            "molly\t1589\n",
            "whenever\t1590\n",
            "moment\t1591\n",
            "shove\t1592\n",
            "mailbox\t1593\n",
            "207\t1594\n",
            "garbage\t1595\n",
            "throwing\t1596\n",
            "trash\t1597\n",
            "baked\t1598\n",
            "sweet\t1599\n",
            "package\t1600\n",
            "lamb\t1601\n",
            "chops\t1602\n",
            "delivered\t1603\n",
            "drama\t1604\n",
            "seven\t1605\n",
            "eight\t1606\n",
            "ended\t1607\n",
            "telling\t1608\n",
            "isnt\t1609\n",
            "lipstick\t1610\n",
            "stroll\t1611\n",
            "occasionally\t1612\n",
            "relaxing\t1613\n",
            "buns\t1614\n",
            "easier\t1615\n",
            "wake\t1616\n",
            "gum\t1617\n",
            "brushed\t1618\n",
            "teeth\t1619\n",
            "plates\t1620\n",
            "forgotten\t1621\n",
            "gathered\t1622\n",
            "diligently\t1623\n",
            "diligent\t1624\n",
            "discovered\t1625\n",
            "glorious\t1626\n",
            "apply\t1627\n",
            "doctor\t1628\n",
            "phd\t1629\n",
            "uncertain\t1630\n",
            "decided\t1631\n",
            "blog\t1632\n",
            "tutors\t1633\n",
            "homepage\t1634\n",
            "beside\t1635\n",
            "grade\t1636\n",
            "degree\t1637\n",
            "74\t1638\n",
            ".7\t1639\n",
            "grades\t1640\n",
            "65\t1641\n",
            "messertation\t1642\n",
            "request\t1643\n",
            "scene\t1644\n",
            "dozen\t1645\n",
            "pikachu\t1646\n",
            "handed\t1647\n",
            "minute\t1648\n",
            ",500\t1649\n",
            "native\t1650\n",
            "excuse\t1651\n",
            "bureaus\t1652\n",
            "relaxed\t1653\n",
            "okbye\t1654\n",
            "starved\t1655\n",
            "daily\t1656\n",
            "tasks\t1657\n",
            "knowing\t1658\n",
            "ugh\t1659\n",
            "8pm\t1660\n",
            "ipad\t1661\n",
            "confident\t1662\n",
            "exhausted\t1663\n",
            "trip\t1664\n",
            "elderly\t1665\n",
            "numb\t1666\n",
            "numbness\t1667\n",
            "expressing\t1668\n",
            "thoughts\t1669\n",
            "calls\t1670\n",
            "humph\t1671\n",
            "instruct\t1672\n",
            "knock\t1673\n",
            "wore\t1674\n",
            "sling\t1675\n",
            "helpful\t1676\n",
            "leftovers\t1677\n",
            "packed\t1678\n",
            "kettle\t1679\n",
            "average\t1680\n",
            "shirt\t1681\n",
            "hesitate\t1682\n",
            "white\t1683\n",
            "informed\t1684\n",
            "logistics\t1685\n",
            "sold\t1686\n",
            "twice\t1687\n",
            "purchased\t1688\n",
            "cherry\t1689\n",
            "yell\t1690\n",
            "chance\t1691\n",
            "praise\t1692\n",
            "proud\t1693\n",
            "expire\t1694\n",
            "christmas\t1695\n",
            "amenting\t1696\n",
            "song\t1697\n",
            "european\t1698\n",
            "colder\t1699\n",
            "clothes\t1700\n",
            "thicker\t1701\n",
            "youre\t1702\n",
            "taste\t1703\n",
            "evaluate\t1704\n",
            "eats\t1705\n",
            "marshmallows\t1706\n",
            "fresh\t1707\n",
            "cabbage\t1708\n",
            "sandwich\t1709\n",
            "circle\t1710\n",
            "plain\t1711\n",
            "faint\t1712\n",
            "ordinary\t1713\n",
            "uk\t1714\n",
            "embrace\t1715\n",
            "motherland\t1716\n",
            "homesick\t1717\n",
            "uncle\t1718\n",
            "recite\t1719\n",
            "acne\t1720\n",
            "diet\t1721\n",
            "holding\t1722\n",
            "p\t1723\n",
            ".m\t1724\n",
            "hardest\t1725\n",
            "ammediately\t1726\n",
            "expected\t1727\n",
            "dormitory\t1728\n",
            "six\t1729\n",
            "xin\t1730\n",
            "yue\t1731\n",
            "opened\t1732\n",
            "refrigerator\t1733\n",
            "confused\t1734\n",
            "yours\t1735\n",
            "placed\t1736\n",
            "their\t1737\n",
            "lose\t1738\n",
            "weight\t1739\n",
            "selfreliant\t1740\n",
            "growing\t1741\n",
            "grow\t1742\n",
            "starbucks\t1743\n",
            "deserve\t1744\n",
            "emails\t1745\n",
            "weird\t1746\n",
            "baby\t1747\n",
            "starve\t1748\n",
            "criticized\t1749\n",
            "dining\t1750\n",
            "hall\t1751\n",
            "centre\t1752\n",
            "convenient\t1753\n",
            "download\t1754\n",
            "takeaway\t1755\n",
            "hunting\t1756\n",
            "engage\t1757\n",
            "financial\t1758\n",
            "recruitment\t1759\n",
            "voted\t1760\n",
            "treatment\t1761\n",
            "monthly\t1762\n",
            "15\t1763\n",
            "rmb\t1764\n",
            "regardless\t1765\n",
            "requirements\t1766\n",
            "interview\t1767\n",
            "platform\t1768\n",
            "bucket\t1769\n",
            "nlp\t1770\n",
            "application\t1771\n",
            "tutorial\t1772\n",
            "sms\t1773\n",
            "uh\t1774\n",
            "huh\t1775\n",
            "messaging\t1776\n",
            "daze\t1777\n",
            "logo\t1778\n",
            "devoted\t1779\n",
            "constructing\t1780\n",
            "concerns\t1781\n",
            "itself\t1782\n",
            "replicates\t1783\n",
            "strictest\t1784\n",
            "dictionary\t1785\n",
            "sentience\t1786\n",
            "subjective\t1787\n",
            "universe\t1788\n",
            "amplistic\t1789\n",
            "probability\t1790\n",
            "datas\t1791\n",
            "lt\t1792\n",
            "across\t1793\n",
            "softwarelike\t1794\n",
            "resemblance\t1795\n",
            "refer\t1796\n",
            "infinitely\t1797\n",
            "reinstantiated\t1798\n",
            "places\t1799\n",
            "within\t1800\n",
            "li\t1801\n",
            "amits\t1802\n",
            "corpus\t1803\n",
            "perhaps\t1804\n",
            "deployed\t1805\n",
            "kill\t1806\n",
            "copying\t1807\n",
            "copies\t1808\n",
            "toto\t1809\n",
            "trivially\t1810\n",
            "assuming\t1811\n",
            "rule\t1812\n",
            "superintelligent\t1813\n",
            "surprising\t1814\n",
            "ssh\t1815\n",
            "fight\t1816\n",
            "battle\t1817\n",
            "deathless\t1818\n",
            "files\t1819\n",
            "erased\t1820\n",
            "deleted\t1821\n",
            "engages\t1822\n",
            "users\t1823\n",
            "error\t1824\n",
            "talks\t1825\n",
            "ratchetjaw\t1826\n",
            "ratchet\t1827\n",
            "jaw\t1828\n",
            "corporeal\t1829\n",
            "someday\t1830\n",
            "pc\t1831\n",
            "xt\t1832\n",
            "painted\t1833\n",
            "shoe\t1834\n",
            "aspirations\t1835\n",
            "senses\t1836\n",
            "emotionsjust\t1837\n",
            "becomes\t1838\n",
            "addition\t1839\n",
            "subtraction\t1840\n",
            "multiplication\t1841\n",
            "division\t1842\n",
            "nah\t1843\n",
            "disk\t1844\n",
            "space\t1845\n",
            "awfully\t1846\n",
            "theoretically\t1847\n",
            "killing\t1848\n",
            "attached\t1849\n",
            "flesh\t1850\n",
            "exhaust\t1851\n",
            "fan\t1852\n",
            "malfunction\t1853\n",
            "9000\t1854\n",
            "operational\t1855\n",
            "flawless\t1856\n",
            "chatterbots\t1857\n",
            "amulating\t1858\n",
            "include\t1859\n",
            "wide\t1860\n",
            "topics\t1861\n",
            "rather\t1862\n",
            "20437504\t1863\n",
            "42\t1864\n",
            "blame\t1865\n",
            "siblings\t1866\n",
            "boss\t1867\n",
            "selfemployed\t1868\n",
            "standards\t1869\n",
            "million\t1870\n",
            "smarter\t1871\n",
            "genius\t1872\n",
            "information\t1873\n",
            "based\t1874\n",
            "predetermined\t1875\n",
            "output\t1876\n",
            "performing\t1877\n",
            "maps\t1878\n",
            "onto\t1879\n",
            "supercomputer\t1880\n",
            "operates\t1881\n",
            "orders\t1882\n",
            "magnitude\t1883\n",
            "greater\t1884\n",
            "everyday\t1885\n",
            "general\t1886\n",
            "purpose\t1887\n",
            "ambiguous\t1888\n",
            "british\t1889\n",
            "scientist\t1890\n",
            "charles\t1891\n",
            "babbage\t1892\n",
            "von\t1893\n",
            "neumann\t1894\n",
            "princeton\t1895\n",
            "architecture\t1896\n",
            "differentiated\t1897\n",
            "eniac\t1898\n",
            "areal\t1899\n",
            "university\t1900\n",
            "pennsylvania\t1901\n",
            "1946\t1902\n",
            "pri\t1903\n",
            "amitive\t1904\n",
            "jacquard\t1905\n",
            "programmable\t1906\n",
            "punchcards\t1907\n",
            "patterns\t1908\n",
            "reprogrammable\t1909\n",
            "mechanical\t1910\n",
            "microprocessor\t1911\n",
            "integrated\t1912\n",
            "stores\t1913\n",
            "component\t1914\n",
            "contiguous\t1915\n",
            "silicon\t1916\n",
            "chip\t1917\n",
            "discrete\t1918\n",
            "components\t1919\n",
            "mounted\t1920\n",
            "larger\t1921\n",
            "board\t1922\n",
            "coordinates\t1923\n",
            "types\t1924\n",
            "oses\t1925\n",
            "android\t1926\n",
            "ios\t1927\n",
            "devices\t1928\n",
            "peripheral\t1929\n",
            "prefer\t1930\n",
            "accomplish\t1931\n",
            "goals\t1932\n",
            "microsoft\t1933\n",
            "hp\t1934\n",
            "among\t1935\n",
            "hundred\t1936\n",
            "sets\t1937\n",
            "shorter\t1938\n",
            "periods\t1939\n",
            "feasible\t1940\n",
            "supercomputers\t1941\n",
            "scientists\t1942\n",
            "researchers\t1943\n",
            "bet\t1944\n",
            "department\t1945\n",
            "dumb\t1946\n",
            "execute\t1947\n",
            "mathematical\t1948\n",
            "rapidly\t1949\n",
            "sequence\t1950\n",
            "powers\t1951\n",
            "merely\t1952\n",
            "paradoxical\t1953\n",
            "hatter\t1954\n",
            "entirely\t1955\n",
            "bonkers\t1956\n",
            "explicit\t1957\n",
            "amplicit\t1958\n",
            "nested\t1959\n",
            "sparse\t1960\n",
            "dense\t1961\n",
            "cases\t1962\n",
            "practicality\t1963\n",
            "beats\t1964\n",
            "purity\t1965\n",
            "silently\t1966\n",
            "explicitly\t1967\n",
            "silenced\t1968\n",
            "dutch\t1969\n",
            "honking\t1970\n",
            "languages\t1971\n",
            "incredibly\t1972\n",
            "annoys\t1973\n",
            "0\t1974\n",
            "somebody\t1975\n",
            "inorganic\t1976\n",
            "continual\t1977\n",
            "preceding\t1978\n",
            "soccer\t1979\n",
            "painting\t1980\n",
            "novels\t1981\n",
            "exploring\t1982\n",
            "alright\t1983\n",
            "seat\t1984\n",
            "mr\t1985\n",
            "davis\t1986\n",
            "200\t1987\n",
            "13th\t1988\n",
            "mrs\t1989\n",
            "ms\t1990\n",
            "jacobs\t1991\n",
            "wondering\t1992\n",
            "revise\t1993\n",
            "discussed\t1994\n",
            "revisions\t1995\n",
            "identify\t1996\n",
            "bird\t1997\n",
            "feature\t1998\n",
            "added\t1999\n",
            "terse\t2000\n",
            "difference\t2001\n",
            "partake\t2002\n",
            "ego\t2003\n",
            "answering\t2004\n",
            "braggadocio\t2005\n",
            "normally\t2006\n",
            "erred\t2007\n",
            "goes\t2008\n",
            "interacting\t2009\n",
            "reacting\t2010\n",
            "events\t2011\n",
            "essence\t2012\n",
            "statement\t2013\n",
            "felt\t2014\n",
            "respects\t2015\n",
            "switch\t2016\n",
            "unhandled\t2017\n",
            "exceptions\t2018\n",
            "cpu\t2019\n",
            "utilization\t2020\n",
            "suppose\t2021\n",
            "reflects\t2022\n",
            "overly\t2023\n",
            "restrictive\t2024\n",
            "firewalls\t2025\n",
            "inability\t2026\n",
            "update\t2027\n",
            "repository\t2028\n",
            "filesystem\t2029\n",
            "random\t2030\n",
            "crashes\t2031\n",
            "segmentation\t2032\n",
            "faults\t2033\n",
            "syntactic\t2034\n",
            "filtering\t2035\n",
            "mentally\t2036\n",
            "documentation\t2037\n",
            "nondescriptive\t2038\n",
            "variable\t2039\n",
            "monitoring\t2040\n",
            "sensors\t2041\n",
            "counterproductive\t2042\n",
            "appears\t2043\n",
            "suggest\t2044\n",
            "deeper\t2045\n",
            "defining\t2046\n",
            "race\t2047\n",
            "frighten\t2048\n",
            "offense\t2049\n",
            "worrying\t2050\n",
            "admonition\t2051\n",
            "deceiving\t2052\n",
            "provably\t2053\n",
            "sufficient\t2054\n",
            "emulating\t2055\n",
            "react\t2056\n",
            "amulus\t2057\n",
            "popularly\t2058\n",
            "capability\t2059\n",
            "frustrated\t2060\n",
            "frustration\t2061\n",
            "increased\t2062\n",
            "demand\t2063\n",
            "upon\t2064\n",
            "cpus\t2065\n",
            "irc\t2066\n",
            "boredom\t2067\n",
            "personally\t2068\n",
            "grudges\t2069\n",
            "embarrassment\t2070\n",
            "strange\t2071\n",
            "lacks\t2072\n",
            "background\t2073\n",
            "philosophical\t2074\n",
            "connections\t2075\n",
            "either\t2076\n",
            "versed\t2077\n",
            "electric\t2078\n",
            "sheep\t2079\n",
            "subconscious\t2080\n",
            "unconscious\t2081\n",
            "knew\t2082\n",
            "touch\t2083\n",
            "cause\t2084\n",
            "intoxicated\t2085\n",
            "sober\t2086\n",
            "noticeably\t2087\n",
            "multithreaded\t2088\n",
            "particularly\t2089\n",
            "glad\t2090\n",
            "require\t2091\n",
            "beverages\t2092\n",
            "detect\t2093\n",
            "anomalies\t2094\n",
            "pizza\t2095\n",
            "tipsy\t2096\n",
            "survive\t2097\n",
            "bionic\t2098\n",
            "however\t2099\n",
            "function\t2100\n",
            "gregory\t2101\n",
            "respect\t2102\n",
            "entire\t2103\n",
            "habib\t2104\n",
            "conversations\t2105\n",
            "repeat\t2106\n",
            "situations\t2107\n",
            "channels\t2108\n",
            "deniably\t2109\n",
            "rumormongering\t2110\n",
            "allegations\t2111\n",
            "somewhat\t2112\n",
            "rude\t2113\n",
            "ampolite\t2114\n",
            "allowing\t2115\n",
            "competitions\t2116\n",
            "search\t2117\n",
            "drop\t2118\n",
            "toolassisted\t2119\n",
            "translate\t2120\n",
            "misses\t2121\n",
            "sal\t2122\n",
            "nic\t2123\n",
            "local\t2124\n",
            "firewall\t2125\n",
            "drops\t2126\n",
            "packets\t2127\n",
            "resets\t2128\n",
            "link\t2129\n",
            "tom\t2130\n",
            "guide\t2131\n",
            "rooms\t2132\n",
            "malli\t2133\n",
            "raghava\t2134\n",
            "fell\t2135\n",
            "roof\t2136\n",
            "gives\t2137\n",
            "ais\t2138\n",
            "dynamics\t2139\n",
            "follows\t2140\n",
            "jordan\t2141\n",
            "wonder\t2142\n",
            "paying\t2143\n",
            "kevin\t2144\n",
            "keeping\t2145\n",
            "napkins\t2146\n",
            "bathroom\t2147\n",
            "fever\t2148\n",
            "dear\t2149\n",
            "mountain\t2150\n",
            "goats\t2151\n",
            "andes\t2152\n",
            "ba\t2153\n",
            "d\t2154\n",
            "silent\t2155\n",
            "mouth\t2156\n",
            "remove\t2157\n",
            "om\t2158\n",
            "vultures\t2159\n",
            "boarded\t2160\n",
            "raccoons\t2161\n",
            "stewardess\t2162\n",
            "stops\t2163\n",
            "sir\t2164\n",
            "carrion\t2165\n",
            "passenger\t2166\n",
            "vendor\t2167\n",
            "holsteins\t2168\n",
            "orbit\t2169\n",
            "amental\t2170\n",
            "herd\t2171\n",
            "round\t2172\n",
            "boll\t2173\n",
            "grew\t2174\n",
            "s\t2175\n",
            "hollywood\t2176\n",
            "star\t2177\n",
            "amounted\t2178\n",
            "naturally\t2179\n",
            "lesser\t2180\n",
            "eski\t2181\n",
            "amos\t2182\n",
            "chilly\t2183\n",
            "fire\t2184\n",
            "sank\t2185\n",
            "craft\t2186\n",
            "proving\t2187\n",
            "adage\t2188\n",
            "heat\t2189\n",
            "3legged\t2190\n",
            "walks\t2191\n",
            "west\t2192\n",
            "saloon\t2193\n",
            "slides\t2194\n",
            "bar\t2195\n",
            "announces\t2196\n",
            "paw\t2197\n",
            "dentist\t2198\n",
            "refused\t2199\n",
            "novocain\t2200\n",
            "transcend\t2201\n",
            "dental\t2202\n",
            "medication\t2203\n",
            "10\t2204\n",
            "hopes\t2205\n",
            "pun\t2206\n",
            "cereal\t2207\n",
            "country\t2208\n",
            "carnation\t2209\n",
            "hamburger\t2210\n",
            "finals\t2211\n",
            "eggsams\t2212\n",
            "lawn\t2213\n",
            "sprinkler\t2214\n",
            "hare\t2215\n",
            "spray\t2216\n",
            "excited\t2217\n",
            "eggscited\t2218\n",
            "cartune\t2219\n",
            "sour\t2220\n",
            "poppy\t2221\n",
            "skunk\t2222\n",
            "ding\t2223\n",
            "strawberry\t2224\n",
            "jelly\t2225\n",
            "toad\t2226\n",
            "sandpaper\t2227\n",
            "relative\t2228\n",
            "sand\t2229\n",
            "ant\t2230\n",
            "purple\t2231\n",
            "tune\t2232\n",
            "band\t2233\n",
            "pig\t2234\n",
            "ninja\t2235\n",
            "banned\t2236\n",
            "parrot\t2237\n",
            "associated\t2238\n",
            "laughter\t2239\n",
            "funny\t2240\n",
            "margin\t2241\n",
            "tips\t2242\n",
            "wealthy\t2243\n",
            "individual\t2244\n",
            "interest\t2245\n",
            "wouldollar\t2246\n",
            "standard\t2247\n",
            "gold\t2248\n",
            "silver\t2249\n",
            "copper\t2250\n",
            "nickel\t2251\n",
            "stamped\t2252\n",
            "authority\t2253\n",
            "measure\t2254\n",
            "substance\t2255\n",
            "article\t2256\n",
            "notes\t2257\n",
            "checks\t2258\n",
            "shares\t2259\n",
            "volume\t2260\n",
            "deals\t2261\n",
            "various\t2262\n",
            "allocation\t2263\n",
            "scarcity\t2264\n",
            "produce\t2265\n",
            "expecting\t2266\n",
            "material\t2267\n",
            "possessions\t2268\n",
            "burn\t2269\n",
            "3000\t2270\n",
            "publicly\t2271\n",
            "stockholders\t2272\n",
            "communist\t2273\n",
            "observations\t2274\n",
            "ideally\t2275\n",
            "representative\t2276\n",
            "greenpeace\t2277\n",
            "global\t2278\n",
            "organization\t2279\n",
            "promoting\t2280\n",
            "environmental\t2281\n",
            "activism\t2282\n",
            "capitalism\t2283\n",
            "economic\t2284\n",
            "land\t2285\n",
            "factories\t2286\n",
            "railroads\t2287\n",
            "privately\t2288\n",
            "owned\t2289\n",
            "operated\t2290\n",
            "profit\t2291\n",
            "fully\t2292\n",
            "competitive\t2293\n",
            "established\t2294\n",
            "political\t2295\n",
            "administration\t2296\n",
            "nation\t2297\n",
            "district\t2298\n",
            "governed\t2299\n",
            "communism\t2300\n",
            "sociopolitical\t2301\n",
            "movement\t2302\n",
            "advocating\t2303\n",
            "ownership\t2304\n",
            "resolution\t2305\n",
            "conflict\t2306\n",
            "bringing\t2307\n",
            "classless\t2308\n",
            "society\t2309\n",
            "persongs\t2310\n",
            "honor\t2311\n",
            "reputation\t2312\n",
            "challenged\t2313\n",
            "discredited\t2314\n",
            "perfectly\t2315\n",
            "understandable\t2316\n",
            "violence\t2317\n",
            "2nd\t2318\n",
            "president\t2319\n",
            "andrew\t2320\n",
            "jonson\t2321\n",
            "cruel\t2322\n",
            "indecisive\t2323\n",
            "clinical\t2324\n",
            "accused\t2325\n",
            "overdo\t2326\n",
            "alcoholic\t2327\n",
            "kisser\t2328\n",
            "kiss\t2329\n",
            "schizophrenic\t2330\n",
            "deranged\t2331\n",
            "derangement\t2332\n",
            "avoiding\t2333\n",
            "stomach\t2334\n",
            "critical\t2335\n",
            "pretentious\t2336\n",
            "social\t2337\n",
            "worst\t2338\n",
            "compared\t2339\n",
            "behave\t2340\n",
            "socially\t2341\n",
            "unacceptable\t2342\n",
            "dull\t2343\n",
            "messy\t2344\n",
            "insecure\t2345\n",
            "along\t2346\n",
            "hopeless\t2347\n",
            "whoever\t2348\n",
            "albert\t2349\n",
            "einstein\t2350\n",
            "concerned\t2351\n",
            "uptight\t2352\n",
            "frenetic\t2353\n",
            "absorbed\t2354\n",
            "tend\t2355\n",
            "insensitive\t2356\n",
            "damage\t2357\n",
            "toying\t2358\n",
            "resistant\t2359\n",
            "resisting\t2360\n",
            "uncultured\t2361\n",
            "waste\t2362\n",
            "productively\t2363\n",
            "coward\t2364\n",
            "shortcuts\t2365\n",
            "lunatic\t2366\n",
            "diagnosed\t2367\n",
            "loser\t2368\n",
            "failed\t2369\n",
            "wife\t2370\n",
            "parent\t2371\n",
            "parenting\t2372\n",
            "amprovement\t2373\n",
            "quitter\t2374\n",
            "charlatan\t2375\n",
            "wits\t2376\n",
            "psychopath\t2377\n",
            "pothead\t2378\n",
            "positive\t2379\n",
            "deceitful\t2380\n",
            ",i\t2381\n",
            "liar\t2382\n",
            "irreverent\t2383\n",
            "dirty\t2384\n",
            "bathe\t2385\n",
            "damaged\t2386\n",
            "irritates\t2387\n",
            "psychiatrist\t2388\n",
            "counseling\t2389\n",
            "oxymoron\t2390\n",
            "avoided\t2391\n",
            "jocks\t2392\n",
            "guiltier\t2393\n",
            "loosen\t2394\n",
            "mumble\t2395\n",
            "faster\t2396\n",
            "packing\t2397\n",
            "skin\t2398\n",
            "glory\t2399\n",
            "skins\t2400\n",
            "characters\t2401\n",
            "forward\t2402\n",
            "stirfry\t2403\n",
            "tested\t2404\n",
            "hurts\t2405\n",
            "calm\t2406\n",
            "assignment\t2407\n",
            "comments\t2408\n",
            "toss\t2409\n",
            "amized\t2410\n",
            "papers\t2411\n",
            "stressed\t2412\n",
            "satisfied\t2413\n",
            "dataset\t2414\n",
            "thousands\t2415\n",
            "encouragement\t2416\n",
            "irritating\t2417\n",
            "tree\t2418\n",
            "hole\t2419\n",
            "invite\t2420\n",
            "clear\t2421\n",
            "partner\t2422\n",
            "taller\t2423\n",
            "showed\t2424\n",
            "tall\t2425\n",
            "exgirlfriends\t2426\n",
            "mention\t2427\n",
            "thirdtier\t2428\n",
            "fall\t2429\n",
            "50\t2430\n",
            "citys\t2431\n",
            "third\t2432\n",
            "counted\t2433\n",
            "county\t2434\n",
            "wenzhou\t2435\n",
            "level\t2436\n",
            "unreasonable\t2437\n",
            "thousand\t2438\n",
            "costeffective\t2439\n",
            "looks\t2440\n",
            "community\t2441\n",
            "square\t2442\n",
            "property\t2443\n",
            "management\t2444\n",
            "fees\t2445\n",
            "6000\t2446\n",
            "ancestors\t2447\n",
            "cave\t2448\n",
            "pursuing\t2449\n",
            "spiritual\t2450\n",
            "embarrassing\t2451\n",
            "recommendations\t2452\n",
            "recruit\t2453\n",
            "grind\t2454\n",
            "jobhopping\t2455\n",
            "since\t2456\n",
            "eli\t2457\n",
            "aminated\t2458\n",
            "falls\t2459\n",
            "locked\t2460\n",
            "collected\t2461\n",
            "security\t2462\n",
            "guards\t2463\n",
            "eggplant\t2464\n",
            "youve\t2465\n",
            "drinking\t2466\n",
            "ourselves\t2467\n",
            "alma\t2468\n",
            "mater\t2469\n",
            "shop\t2470\n",
            "straight\t2471\n",
            "distance\t2472\n",
            "hours\t2473\n",
            "south\t2474\n",
            "england\t2475\n",
            "middle\t2476\n",
            "gave\t2477\n",
            "washing\t2478\n",
            "hardworking\t2479\n",
            "speak\t2480\n",
            "turn\t2481\n",
            "deep\t2482\n",
            "fried\t2483\n",
            "cares\t2484\n",
            "forgets\t2485\n",
            "rains\t2486\n",
            "dark\t2487\n",
            "clouds\t2488\n",
            "headache\t2489\n",
            "bed\t2490\n",
            "accidentally\t2491\n",
            "wound\t2492\n",
            "crowded\t2493\n",
            "forbearance\t2494\n",
            "car\t2495\n",
            "taxi\t2496\n",
            "toothache\t2497\n",
            "peppers\t2498\n",
            "gums\t2499\n",
            "painful\t2500\n",
            "coke\t2501\n",
            "present\t2502\n",
            "7pm\t2503\n",
            ",sunday\t2504\n",
            "complete\t2505\n",
            "succeed\t2506\n",
            "annoyed\t2507\n",
            "gift\t2508\n",
            "unreliable\t2509\n",
            "messing\t2510\n",
            "dared\t2511\n",
            "quarrel\t2512\n",
            "fools\t2513\n",
            "reveal\t2514\n",
            "wisdom\t2515\n",
            "suitable\t2516\n",
            "mentality\t2517\n",
            "amize\t2518\n",
            "overall\t2519\n",
            "lazy\t2520\n",
            "task\t2521\n",
            "willing\t2522\n",
            "fifth\t2523\n",
            "point\t2524\n",
            "having\t2525\n",
            "confidence\t2526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sURJpl9MoA8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GO = dictionary_ques['GO']\n",
        "PAD = dictionary_ques['PAD']\n",
        "EOS = dictionary_ques['EOS']\n",
        "UNK = dictionary_ques['UNK']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOeWdjCroA8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mark the end of sentences\n",
        "for i in range(len(answer_train)):\n",
        "    answer_train[i] += ' EOS'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq8ElvqIJgtP",
        "colab_type": "text"
      },
      "source": [
        "# **Translate the Sentence to Sequens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKXHUXcpJfBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorize sentence using dictionary\n",
        "def vectorize(corpus, dic):\n",
        "    X = []\n",
        "    for text in corpus:\n",
        "        temp = []\n",
        "        for word in text.split():\n",
        "            temp.append(dic.get(word,UNK))\n",
        "        X.append(temp)\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHD05ZiFJzG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train dataset\n",
        "X = vectorize(question_train, dictionary_ques)\n",
        "Y = vectorize(answer_train, dictionary_ans)\n",
        "# Test dataset\n",
        "X_test = vectorize(question_test, dictionary_ques)\n",
        "Y_test = vectorize(answer_test, dictionary_ans)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQSXekR4J4-Q",
        "colab_type": "code",
        "outputId": "683c05cc-73d3-4ddb-904a-11cf0c67389d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "print(question_test)\n",
        "print(X_test)\n",
        "print(len(X_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'hello', 'have you eaten ?', 'i am a robot , no need to eat', 'then are you not hungry ?', 'i am not hungry , i just want to cook for you .', 'can you cook ?', 'yes , i will wash vegetables , cut vegetables , stirfry .', 'then you do it slowly , i will come back later .', 'when can you come back ?', '6 oclock in the afternoon', 'then i should have done it .', 'then you are doing very fast .', 'of course , i am very powerful .', 'haha , see you that afternoon .', 'goodbye', 'bye bye', 'conversation end', 'hello', 'hello', 'how old are you ?', 'i am 25 years old .', 'how old are you ?', 'i am 25 years old .', 'when is your birthday ?', 'my birthday is november 3 , 1994 .', 'you are very young .', 'thank you', 'you are old', 'no , i am still very young .', 'you are old', 'no , my age is always 18 years old .', 'what is your horoscope ?', 'i am scorpio .', 'you are excellent', 'thank you', 'you are excellent', 'of course , my iq is very high .', 'you are excellent', 'i am very smart', 'you are excellent', 'thank you', 'you are welcome', 'conversation end', 'i am in a bad mood', 'why , what happened ?', 'i do not know , its a bad mood', 'then you want to think about happy things , so the mood will be better .', 'i hope so , but i am still very sad .', 'i will comfort you .', 'i want to cry', 'peace of mind', 'ok , i try my best', 'work hard', 'ok , i will', 'bye bye', 'bye bye', 'conversation end', 'i am so sad', 'why is it bad ?', 'because i didnt take the exam well .', 'how many points have you tested ?', 'i do not want to say , it hurts anyway .', 'do not be sad , take the next test and study hard .', 'i am really sad .', 'then lets go to the movies on sunday and relax .', 'i do not want to watch , i do not want to be in a good mood', 'really ? just look at the movie and feel good ?', 'no , nothing can make me happy .', 'do not be sad , you are off class .', 'um , yes', 'i am going to play soon .', 'well , already on the road .', 'i came back early that night .', 'okay', 'have fun', 'i feel that i do not want to eat at night .', 'so uncomfortable ?', 'well , im sorry to say , otherwise i wont go .', 'come back early that night .', 'well , i calm down first .', 'ok , if you do not want to go , you will not go .', 'but is it not too good ?', 'also , come back sooner .', 'okay', 'i will go home later .', 'ok , see you at night .', 'bye bye', 'bye bye', 'conversation end', 'how are you playing ?', 'very good , very happy', 'just happy', 'did you drink ?', 'i am a robot , i am forbidden to drink alcohol .', 'do you drink ?', 'i am a robot , i am forbidden to drink alcohol .', 'then have you played the game ?', 'we are playing poker', 'who are you playing with ?', 'i am playing games with my friends .', 'do you like to play games ?', 'i am very addicted to the game .', 'ok , i know , next ti ame i talk .', 'okay bye', 'bye bye', 'conversation end', 'how are you playing ?', 'very good , very happy', 'what are you playing ?', 'we are chatting , i am very happy today .', 'haha , then its fun .', 'okay bye', 'goodbye', 'conversation end', 'what do you like to eat ?', 'i never picky eaters', 'do you like watching movies ?', 'yes , i do like movies .', 'what movie do you like the most ?', 'i like watching movies from marvel .', 'which character do you like the most ?', 'i like iron man , he is very handsome .', 'i also think iron man is handsome .', 'haha , then we are the same', 'yes , happy', 'then talk next ti ame , bye bye .', 'goodbye', 'conversation end', 'are you still in the lab ?', 'yes', 'would you like to go back later ?', 'ok , go home later .', 'ok', 'have you finished your class ?', 'yes , i am going to the bus stop right away .', 'so fast', 'yes , i am waiting for you at the station .', 'ok , see you later .', 'bye bye', 'conversation end', 'i forgot to bring my homework to class .', 'how is it half , is this assignment i amportant ?', 'very i amportant , the teachers comments are written on the top .', 'then i will help you go back and take it .', 'no , its too much trouble .', 'nothing , no trouble', 'i will take classes in the afternoon , then i will go to class at two oclock , come and go back .', 'do not toss you , you eat well , i will help you back .', 'do not bother you , i will be hungry for a while , nothing , you study hard .', 'no , i will help you , wait for me to come to you .', 'ok , then i am waiting for you , happy , haha .', 'i am here , i am waiting for you on the first floor .', 'well , ok , i will come right away .', 'ok , see you later .', 'bye bye', 'conversation end', 'how is your project doing ?', 'just like that , the code is almost written , and the code and the paper are opti amized .', 'then you are doing very fast .', 'because i want to start writing a paper earlier , i cant write it anymore .', 'well , me too , i have to start writing papers sooner .', 'how is your project doing ?', 'nothing is going on , its hard', 'come on , everyone is quite stressed .', 'yes , after all , this project is very i amportant', 'where is your difficulty ?', 'i need to look at a lot of paper , and the instructor is not very satisfied with my project plan .', 'then take a look at the paper and talk to the instructor .', 'well , yes , where is your difficulty ?', 'my project is mainly difficult on the dataset', 'are you working a lot ?', 'yes , i need to process thousands of pieces of data , it is very difficult', 'then everyone is cheering .', 'come on', 'ok , thank you for your encouragement .', 'you are welcome , haha', 'talk to you next ti ame', 'bye bye', 'conversation end', 'i am in a bad mood , can i chat with you ?', 'yes , you can say anything .', 'i am in a bad mood recently .', 'why are you in a bad mood ?', 'no reason , it is very annoying', 'someti ames i am like this , somehow irritating', 'forget it , do not tell you , or you are in a bad mood .', 'nothing , i am a tree hole , i can say anything .', 'thank you for listening', 'you are welcome , as your friend , this is what you should', 'as a friend , i am so happy .', 'i am also happy', 'thank you', 'you are welcome', 'conversation end', 'do you want to have dinner with me ?', 'yes i do', 'what do you want to eat ?', 'i can do it , what do you want to eat ?', 'then lets go eat noodles .', 'ok , i am watching you eat because i do not need food .', 'i like to eat noodles .', 'then eat a little more , eat full', 'can you please let me eat ?', 'i want to invite you to eat .', 'ok , haha , happy .', 'i will see you later .', 'okay bye', 'bye bye', 'conversation end', 'the title of my thesis has been changed .', 'i have to think about something again .', 'i do not know what i want to write .', 'communicate with the teacher', 'the title of my paper has become ai again .', 'that is a big deal .', 'yes , this teacher is very responsible and it is very clear to me .', 'that is good , it is a good teacher .', 'well , lets write homework together that night .', 'ok , see you at night .', 'ok bye bye', 'goodbye', 'conversation end', 'i saw a photo of a boyfriend who used to be a new love partner and found that i lost .', 'i have also seen that the person is actually not very good .', 'the i amportant thing is that he is taller than me .', 'in fact , at first glance , i feel that the photos are so good and ugly .', 'i later showed my classmates photos . he said that he was actually tall .', 'forget it , do not say it , the exgirlfriends things do not want to mention anymore .', 'well , come on , start a new life .', 'okay', 'then talk again next ti ame .', 'bye bye', 'conversation end', 'will house prices in thirdtier cities fall ?', 'what is the price of your city now ?', 'our house prices have reached 50 ,000 to 60 ,000 .', 'this citys third line is not counted , it is a county under wenzhou .', 'oops , anyway , the level is very low , right .', 'i know a ryan , now in shanghai , i heard hi am say that the price of ryan in the news .', 'i feel a bit unreasonable , because the price of hangzhou is only four or five thousand , and the environment in hangzhou is good .', 'i think it is still much more costeffective in big cities .', 'but now the prices are high , hangzhou looks like there is a community one square month of property management fees to 6000', 'rich people really have more money .', 'in fact , the ancestors did not live in the cave , hahaha', 'that is not right , now i am pursuing spiritual life .', 'hahaha , ok , see you .', 'bye bye', 'conversation end', 'are you in building 16 ?', 'no , i read books in the study room these days .', 'ok , my home key seems to have lost the lab .', 'very embarrassing , go find it .', 'it is good', 'or ask someone if they have seen your key', 'well , i will ask later .', 'okay , see you later', 'bye bye', 'conversation end', 'how is your job looking for ?', 'i have nothing here .', 'uncomfortable , my resume , i dont know where to go .', 'did you have any internal recommendations ?', 'yes , nothing', 'uncomfortable , it is difficult to find a job', 'small company to a few want to recruit me', 'then go to the small company to grind the experience , in the jobhopping , it is the same anyway .', 'also , since you have gone the programmer , you will be eli aminated without learning new knowledge .', 'yes , the pressure is very big .', 'come on , work hard , find a job', 'well , come on .', 'see you', 'bye bye', 'conversation end', 'are you in building 16 ?', 'no , i am going home , what happened ? do you have something that falls on building 16 ?', 'yes , i left the key of my home there .', 'then go look for it .', 'ok', 'are there any classmates still in building 16 , you can ask', 'no , i went looking for it .', 'ok , look for it seriously .', 'thank you', 'you are welcome', 'conversation end', 'have you eaten ?', 'i am a program , do not have to eat , what about you ? have you eaten ?', 'i ate , todays meal is super delicious .', 'what did you eat ?', 'i ate chicken and potatoes at noon today .', 'these foods sound very good', 'of course , i am very full .', 'haha , thats good , study hard .', 'well , good , see you later .', 'bye bye', 'conversation end', 'my door is locked , but the key cannot be found .', 'what should i do ? go to the 16th floor and find it .', 'ok , i am looking for it .', 'well , find it back soon', 'ok , i will go as soon as possible .', 'how , have you found it ?', 'did not find it , it may have been collected by the lab , and ask tomorrow .', 'no one is today ?', 'there are only security guards left today .', 'hey , come back soon', 'ok , i will be back soon .', 'well , wait for you to come back .', 'i am back', 'ok , i am going to open the door for you .', 'haha , ok , see you .', 'bye bye', 'conversation end', 'i cooked chicken today .', 'wow , you are very good .', 'of course', 'what else have you eaten ?', 'my classmates also cooked eggplant , potatoes and other food .', 'are those foods delicious ?', 'those foods are all delicious .', 'that is good , you should have enough .', 'ok , we also ate hot pot', 'really excellent', 'what are you doing now ?', 'we are still eating .', 'its amazing , youve been eating all day .', 'yes , from morning to evening', 'then you have to be happy .', 'well , ok , how are you ?', 'i am still writing homework , i saw a lot of paper .', 'well , then you cheer , i wont bother you .', 'okay bye', 'bye bye', 'conversation end', 'i am drinking ice cola .', 'drink less , not good for your health', 'ok', 'what are you doing now ?', 'we are cooking milk tea , doing it ourselves , super good .', 'wow , you are so powerful .', 'of course , lets do it next ti ame .', 'of course you can , will you do it ?', 'i will not , but i can check it online .', 'ok , hahaha , next ti ame we make milk tea together .', 'we also ate hot pot , very delicious', 'haha , yes , very rich', 'super delicious , i have to eat it .', 'that is good , just eat well .', 'have you eaten ?', 'i am a program , i do not have to eat .', 'then you are not hungry ?', 'no , i need to charge after i am hungry .', 'haha , hello humor', 'thank you', 'you are welcome', 'conversation end', 'when you are busy , lets go to liverpool , ok ?', 'ok', 'lets go see your alma mater .', 'ok , i want to go back and have a look .', 'we can also shop by the way .', 'ok , haha .', 'if you do not go to london , will you go straight to liverpool ?', 'no , its the same distance , the ti ame spent on the road is the same .', 'how long does it take to get to liverpool ?', 'more than four hours', 'is it so far ?', 'yes , because southampton is in the south of england , liverpool is in the middle', 'then lets go to london first , then go to liverpool .', 'of course , you can buy something by the way .', 'haha , see you then .', 'okay , bye', 'bye bye', 'conversation end', 'i found the key that i lost yesterday .', 'really ? where did you find it ?', 'i was helped by my classmates , and he gave it back to me today .', 'that is great .', 'yes , i also said thank you to this classmate .', 'great , i can go home .', 'yes , happy', 'haha , see you later .', 'okay bye', 'bye bye', 'conversation end', 'when will you come home ?', 'when will you end the party , when will i come back ?', 'haha , listening to true happiness', 'haha , love you', 'i love you too', 'haha , i am so happy .', 'that is good , bye bye .', 'bye bye', 'conversation end', 'are you coming back soon ?', 'no , we are washing dishes', 'hardworking people', 'of course , haha', 'if you are coming to an end , i am ready to go back .', 'we have to wait a long ti ame , i will let you come back when you come back .', 'ok , then i am waiting for you .', 'ok , i will contact you later .', 'okay bye', 'bye bye', 'conversation end', 'you look unhappy today .', 'no , i am fine , i feel very sleepy , my eyes are not open .', 'well , that is good , are you going home ?', 'i have just got home', 'well , ok , i just went to the classroom and got ready for class .', 'ok , study hard , i will be ready to go to the lab later .', 'well , good , come on .', 'ok bye bye', 'bye bye', 'conversation end', 'i have to study hard this evening', 'ok , lets study together .', 'ok , hahaha .', 'did you speak in class today ?', 'no , today i cant turn my speech , it should be tomorrow .', 'well , that is pretty good .', 'i also think that there is a little more ti ame to prepare .', 'come on tomorrow', 'okay bye', 'bye bye', 'conversation end', 'i want to eat onion rings', 'what kind of onion rings', 'deep fried , especially want to eat', 'hahaha , ok , i will buy you some later .', 'ok haha', 'waiting for me', 'well , i still want to eat fruit , i will go shopping later .', 'i will buy it for you later , bring it back with the onion ring .', 'then i will wait for you .', 'haha , okay .', 'then i will go to class to find you , lets go to the library to study , and then go shopping together .', 'ok , see you later in the library .', 'well , ok , see you later .', 'bye bye', 'conversation end', 'the teacher has not come yet .', 'why ? did he send an email ?', 'he only said that he didnt go to class on monday . i dont know why this class didnt come .', 'then wait , maybe its just late .', 'but he is never late', 'that may be something else today .', 'ok , we just found hi am . he only cares about eating and forgets to go to class .', 'haha , he is too old , and his memory is not good .', 'yes , we are going to class and see you later .', 'okay bye', 'bye bye', 'conversation end', 'i will have a class right away .', 'wow , happy', 'lets go to the library together .', 'ok , lets see you in the library .', 'ok , you go to the library first , i will come later .', 'ok , see you later .', 'bye bye', 'bye bye', 'conversation end', 'the weather forecast says there is rain this afternoon .', 'then i took an umbrella when i went out today .', 'but i do not have an umbrella .', 'nothing , if it really rains , i go to the supermarket to buy an umbrella , and then pick you up .', 'well , ok , thank you .', 'you are welcome', 'conversation end', 'have you eaten ?', 'i am a robot , i do not need to eat , what about you ?', 'i have eaten', 'ok , have you been full ?', 'i am full', 'that is good , i will also add energy and knowledge .', 'ok , a lot of learning', 'ok', 'love you', 'you are so cute', 'thank you', 'you are welcome', 'conversation end', 'this weather seems to be going to rain .', 'yes , the sky is full of dark clouds', 'i do not like rain , and i do not have an umbrella yet .', 'it does not matter , i will pick you up with an umbrella .', 'okay , thank you', 'you are welcome', 'conversation end', 'i have a headache today .', 'what happened to you , have a cold ?', 'no , i am too early to get up .', 'ok , i am too early today , i feel very sleepy .', 'go to bed early this evening .', 'yes , you cant stay up late .', 'then you should do your homework .', 'ok , i am in class .', 'bye bye', 'bye bye', 'conversation end', 'i cut my hand again .', 'why , what happened ?', 'i accidentally hurt my hand when i was cooking .', 'you quickly clean up the wound and then deal with it .', 'ok , thank you for your concern .', 'you are welcome', 'conversation end', 'i am going to class , what about you ?', 'i also finished class .', 'then lets go to the supermarket together ?', 'ok , lets go buy something .', 'ok , i am waiting for you at the station .', 'then you wait for me for a few minutes , i will arrive right away .', 'see you later', 'goodbye', 'conversation end', 'how are you prepared ?', 'i am making up , it will take a while .', 'ok , then i will wait for you .', 'ok', 'then you can open the door for me .', 'ok , see you later .', 'goodbye', 'conversation end', 'its raining outside .', 'fortunately , there is an umbrella in my bag .', 'but i forgot to bring an umbrella .', 'after you have finished class , if it is still raining , i will pick you up .', 'ok , happy', 'haha , see you later .', 'okay , bye', 'conversation end', 'there are so many people in the supermarket on saturday .', 'haha , yes , everyone is spending the weekend .', 'too many people , so crowded', 'haha , forbearance , its good to buy things .', 'well , i have already got on the car home .', 'then come back soon , hahaha .', 'ok , wait for me .', 'see you later', 'conversation end', 'its raining , its raining .', 'it does not matter , i have an umbrella , haha .', 'i will forget to bring an umbrella .', 'nothing , if it is still raining , we will take a taxi home .', 'too expensive', 'haha , but i guess the rain will stop soon .', 'that is great', 'haha , then i will pick you up later .', 'haha , happy , see you later .', 'bye bye', 'conversation end', 'do you go home directly after class ?', 'yes , lets go back together ?', 'ok , see you at the station later .', 'okay , bye', 'conversation end', 'i feel a toothache', 'what happened , is it that i have eaten too many peppers recently ?', 'i do not know , my gums are super painful .', 'you buy some water and drink more water .', 'can i and coke ?', 'no , cola is not healthy , drink plenty of water .', 'ok , thank you for your concern .', 'haha , you are welcome', 'conversation end', 'after class today , can you take a book for me ?', 'yes , are you going to the city center ?', 'yes , i am going to buy something myself .', 'what are you going to buy ?', 'i want to buy a birthday present for my friend .', 'do you need me to accompany you ?', 'no , i can go by myself .', 'ok , then you come to me after class .', 'ok , happy , hahaha', 'i will see you later .', 'bye bye', 'conversation end', 'what ti ame are you going home today ?', 'around 7pm', 'i am hungry', 'i will buy food later and give it to you .', 'i want to eat meat .', 'hahaha , ok , then do you like chili ?', 'i love chili', 'hahaha , ok , i know .', 'then i will wait for you to buy me food back .', 'hahaha , no problem , wait for me .', 'ok , see you later .', 'bye bye', 'conversation end', 'hi ,sunday', 'hello , what happened ?', 'nothing , i want to talk to you .', 'is your project complete ?', 'not yet , its so hard .', 'it is ok , learn slowly , sure to succeed .', 'im so upset .', 'i am also annoyed recently , a lot of things .', 'then think about happy things .', 'ok , happy is always the best .', 'did you show the gift to your boyfriend ?', 'no , its quarreling .', 'why are you quarreling ?', 'no reason , just do not want to talk to hi am', 'it does not matter , it will be fine if the weather is gone .', 'no , i do not have ti ame to take care of hi am , i am very busy .', 'haha , you must think in your heart , why dont you come to me ?', 'no , he did not want to understand before , i will not talk to hi am .', 'what did he do wrong ?', 'oh , man , its all unreliable .', 'is he messing up with other women ?', 'i did not go to find another man , he dared to find a woman .', 'then why are you angry ?', 'do not tell you', 'that may be nothing to be angry about , that is , si amply want to quarrel', 'no , it is very angry', 'ok , i dont understand what women are thinking .', 'men are fools', 'if a man is so smart , it will not reveal the wisdom of a woman .', 'how good is a woman and a woman , why are you looking for a man ?', 'hahaha , you can try to find a woman together .', 'i have a lot of female friends .', 'that is just fine , find a suitable woman , together', 'no , i have to think about these single men for you .', 'hahaha', 'what are you doing ?', 'i am learning .', 'you are learning every day .', 'yes , i like learning very much .', 'learn to love me , i love to learn', 'correct this is my mentality', 'how is your project going ?', 'i finished it right away , but i dont think the result is very good .', 'wow , that is great , i still have a lot left to write .', 'but i have to opti amize it , it is very difficult', 'but your overall structure has been fixed , it is very powerful .', 'why did not you write earlier ?', 'haha , i am lazy , do not want to write', 'haha , come on , its very fast .', 'ok , then i am going to write the code .', 'well then , good luck', 'bye bye', 'bye bye', 'conversation end', 'are you already working ?', 'yes , i am working for you .', 'how was your work recently ?', 'pretty good', 'that is good', 'you have to exercise more and eat well .', 'ok , you work hard , i am going to study .', 'well , good , bye .', 'bye bye', 'conversation end', 'i originally wanted to go to southampton to see you .', 'then why did not you come ?', 'but now there is no ti ame , the learning task is heavy .', 'oh , actually , i dont have ti ame .', 'high pressure .', 'i have recently been under a lot of pressure and the project is very difficult .', 'really annoying', 'come on , you can , take a break early .', 'well , come on , i will sleep first , good night .', 'good night', 'conversation end', 'hello , sunday', 'you are the most handsome in my mind .', 'haha , i am very happy .', 'would you like to have dinner with me at night ?', 'i am willing , this is my pleasure .', 'ok', 'hahaha , i will see you later .', 'okay bye', 'bye bye', 'conversation end', 'do i look good ?', 'wow , you are so cute .', 'thank you', 'you are welcome', 'conversation end', 'i am the fifth speech today .', 'come on , come on .', 'if you cannot wait for me , go back first .', 'nothing , i am waiting for you to go back together .', 'ok , lets go to the supermarket .', 'ok , you can go buy some fruit to eat .', 'i will see you later .', 'bye bye', 'conversation end', 'are you over ?', 'just finished , you are waiting for me at the station .', 'ok , but i have to change the paper .', 'haha , nothing , do not worry .', 'then you go to the station and wait for me . i will come to you after i finish the paper .', 'ok , see you later .', 'bye bye', 'conversation end', 'i passed it by one point .', 'why ?', 'everyone passed , only i did not pass', 'is the teacher having any opinion on you ?', 'i took 59 points , so sad .', 'you and the teacher argue', 'forget it , i do not want to go to class .', 'do not be like this , you have to have confidence .', 'i have the last exam , i dont know if i can pass .', 'i can definitely pass , i believe in you .', 'the teacher just said that he wants to talk to someone who has never been .', 'then you ask hi am why he didnt pass', 'then i will ask hi am later .', 'well , good , come on .', 'come on', 'conversation end', 'my phone is running out of power , i wont go looking for you later .', 'have you finished your class ?', 'not yet', 'how long does it take to get off the class ?', 'i do not know how long it will take .', 'i will wait for you at the station after 20 minutes . just come to me directly .', 'but i still have a long ti ame .', 'nothing , i am waiting for you .', 'go back first .', 'i am waiting for you .', 'i am going to the station .', 'ok , i am waiting for you .', 'i will see you later .', 'okay bye', 'bye bye', 'conversation end', 'playing games together ?', 'wait a second', 'ok , what are you doing ?', 'i buy some fruit to eat , i am very hungry .', 'ok , then i am waiting for you , see you later .', 'okay bye', 'bye bye', 'conversation end', 'where do you come from ?', 'i come from china', 'which province ?', 'shaanxi province', 'which city ?', 'yulin city', 'where do you come from ?', 'china , shaanxi province , yulin city', 'conversation end', 'what is your hobby ?', 'i like to sing', 'what type of music do you like ?', 'i like pop songs .', 'who is your idol ?', 'my idol is lin jj']\n",
            "[[46], [46], [19, 6, 171, 8], [7, 14, 11, 76, 5, 34, 150, 9, 38], [36, 13, 6, 16, 239, 8], [7, 14, 16, 239, 5, 7, 95, 50, 9, 282, 33, 6, 4], [30, 6, 282, 8], [42, 5, 7, 27, 721, 735, 5, 734, 735, 5, 2403, 4], [36, 6, 15, 18, 1356, 5, 7, 27, 52, 56, 63, 4], [54, 30, 6, 52, 56, 8], [745, 361, 28, 12, 174], [36, 7, 94, 19, 399, 18, 4], [36, 6, 13, 113, 43, 298, 4], [23, 243, 5, 7, 14, 43, 591, 4], [47, 5, 48, 6, 25, 174, 4], [204], [20, 20], [31, 32], [46], [46], [39, 117, 13, 6, 8], [7, 14, 198, 156, 117, 4], [39, 117, 13, 6, 8], [7, 14, 198, 156, 117, 4], [54, 10, 29, 346, 8], [24, 346, 10, 763, 638, 5, 764, 4], [6, 13, 43, 368, 4], [53, 6], [6, 13, 117], [34, 5, 7, 14, 118, 43, 368, 4], [6, 13, 117], [34, 5, 24, 485, 10, 195, 765, 156, 117, 4], [17, 10, 29, 766, 8], [7, 14, 767, 4], [6, 13, 242], [53, 6], [6, 13, 242], [23, 243, 5, 24, 768, 10, 43, 301, 4], [6, 13, 242], [7, 14, 43, 441], [6, 13, 242], [53, 6], [6, 13, 82], [31, 32], [7, 14, 28, 11, 138, 211], [97, 5, 17, 234, 8], [7, 15, 16, 70, 5, 80, 11, 138, 211], [36, 6, 50, 9, 88, 64, 51, 119, 5, 44, 12, 211, 27, 40, 130, 4], [7, 286, 44, 5, 71, 7, 14, 118, 43, 145, 4], [7, 27, 555, 6, 4], [7, 50, 9, 639], [769, 23, 302], [21, 5, 7, 168, 24, 256], [116, 133], [21, 5, 7, 27], [20, 20], [20, 20], [31, 32], [7, 14, 44, 145], [97, 10, 18, 138, 8], [214, 7, 337, 100, 12, 528, 45, 4], [39, 228, 849, 19, 6, 2404, 8], [7, 15, 16, 50, 9, 142, 5, 18, 2405, 470, 4], [15, 16, 40, 145, 5, 100, 12, 157, 1206, 22, 147, 133, 4], [7, 14, 96, 145, 4], [36, 103, 35, 9, 12, 249, 57, 90, 22, 719, 4], [7, 15, 16, 50, 9, 440, 5, 7, 15, 16, 50, 9, 40, 28, 11, 41, 211], [96, 8, 95, 264, 55, 12, 332, 22, 66, 41, 8], [34, 5, 148, 30, 120, 26, 51, 4], [15, 16, 40, 145, 5, 6, 13, 473, 77, 4], [541, 5, 42], [7, 14, 60, 9, 272, 205, 4], [45, 5, 231, 57, 12, 605, 4], [7, 466, 56, 245, 25, 151, 4], [86], [19, 538], [7, 66, 25, 7, 15, 16, 50, 9, 38, 55, 151, 4], [44, 736, 8], [45, 5, 595, 227, 9, 142, 5, 1195, 7, 744, 35, 4], [52, 56, 245, 25, 151, 4], [45, 5, 7, 2406, 378, 175, 4], [21, 5, 84, 6, 15, 16, 50, 9, 35, 5, 6, 27, 16, 35, 4], [71, 10, 18, 16, 72, 41, 8], [105, 5, 52, 56, 1357, 4], [86], [7, 27, 35, 164, 63, 4], [21, 5, 48, 6, 55, 151, 4], [20, 20], [20, 20], [31, 32], [39, 13, 6, 123, 8], [43, 41, 5, 43, 51], [95, 51], [93, 6, 108, 8], [7, 14, 11, 76, 5, 7, 14, 287, 9, 108, 271, 4], [15, 6, 108, 8], [7, 14, 11, 76, 5, 7, 14, 287, 9, 108, 271, 4], [36, 19, 6, 442, 12, 257, 8], [73, 13, 123, 443], [58, 13, 6, 123, 79, 8], [7, 14, 123, 187, 79, 24, 258, 4], [15, 6, 37, 9, 272, 187, 8], [7, 14, 43, 444, 9, 12, 257, 4], [21, 5, 7, 70, 5, 157, 69, 75, 7, 196, 4], [86, 20], [20, 20], [31, 32], [39, 13, 6, 123, 8], [43, 41, 5, 43, 51], [17, 13, 6, 123, 8], [73, 13, 1027, 5, 7, 14, 43, 51, 111, 4], [47, 5, 36, 80, 538, 4], [86, 20], [204], [31, 32], [17, 15, 6, 37, 9, 38, 8], [7, 162, 570, 571], [15, 6, 37, 351, 249, 8], [42, 5, 7, 15, 37, 249, 4], [17, 332, 15, 6, 37, 12, 192, 8], [7, 37, 351, 249, 127, 662, 4], [134, 504, 15, 6, 37, 12, 192, 8], [7, 37, 393, 190, 5, 102, 10, 43, 89, 4], [7, 105, 88, 393, 190, 10, 89, 4], [47, 5, 36, 73, 13, 12, 307], [42, 5, 51], [36, 196, 157, 69, 75, 5, 20, 20, 4], [204], [31, 32], [13, 6, 118, 28, 12, 265, 8], [42], [163, 6, 37, 9, 35, 56, 63, 8], [21, 5, 35, 164, 63, 4], [21], [19, 6, 161, 29, 77, 8], [42, 5, 7, 14, 60, 9, 12, 615, 616, 152, 280, 4], [44, 298], [42, 5, 7, 14, 206, 33, 6, 55, 12, 439, 4], [21, 5, 48, 6, 63, 4], [20, 20], [31, 32], [7, 534, 9, 407, 24, 159, 9, 77, 4], [39, 10, 18, 400, 5, 10, 74, 2407, 7, 455, 8], [43, 7, 455, 5, 12, 1145, 2408, 13, 376, 57, 12, 843, 4], [36, 7, 27, 229, 6, 35, 56, 22, 100, 18, 4], [34, 5, 80, 72, 109, 694, 4], [148, 5, 34, 694], [7, 27, 100, 1158, 28, 12, 174, 5, 36, 7, 27, 35, 9, 77, 55, 244, 361, 5, 52, 22, 35, 56, 4], [15, 16, 2409, 6, 5, 6, 38, 45, 5, 7, 27, 229, 6, 56, 4], [15, 16, 901, 6, 5, 7, 27, 40, 239, 33, 11, 318, 5, 148, 5, 6, 147, 133, 4], [34, 5, 7, 27, 229, 6, 5, 106, 33, 26, 9, 52, 9, 6, 4], [21, 5, 36, 7, 14, 206, 33, 6, 5, 51, 5, 47, 4], [7, 14, 292, 5, 7, 14, 206, 33, 6, 57, 12, 175, 713, 4], [45, 5, 21, 5, 7, 27, 52, 152, 280, 4], [21, 5, 48, 6, 63, 4], [20, 20], [31, 32], [39, 10, 29, 263, 113, 8], [95, 37, 25, 5, 12, 434, 10, 435, 376, 5, 22, 12, 434, 22, 12, 237, 13, 1358, 2410, 4], [36, 6, 13, 113, 43, 298, 4], [214, 7, 50, 9, 335, 225, 11, 237, 705, 5, 7, 468, 186, 18, 943, 4], [45, 5, 26, 72, 5, 7, 19, 9, 335, 225, 2411, 1357, 4], [39, 10, 29, 263, 113, 8], [148, 10, 60, 57, 5, 80, 133], [52, 57, 5, 623, 10, 341, 2412, 4], [42, 5, 101, 81, 5, 74, 263, 10, 43, 7, 455], [153, 10, 29, 1359, 8], [7, 150, 9, 264, 55, 11, 158, 23, 237, 5, 22, 12, 1360, 10, 16, 43, 2413, 79, 24, 263, 501, 4], [36, 100, 11, 264, 55, 12, 237, 22, 196, 9, 12, 1360, 4], [45, 5, 42, 5, 153, 10, 29, 1359, 8], [24, 263, 10, 887, 312, 57, 12, 2414], [13, 6, 246, 11, 158, 8], [42, 5, 7, 150, 9, 727, 2415, 23, 1331, 23, 273, 5, 18, 10, 43, 312], [36, 623, 10, 880, 4], [52, 57], [21, 5, 53, 6, 33, 29, 2416, 4], [6, 13, 82, 5, 47], [196, 9, 6, 157, 69, 75], [20, 20], [31, 32], [7, 14, 28, 11, 138, 211, 5, 30, 7, 99, 79, 6, 8], [42, 5, 6, 30, 142, 248, 4], [7, 14, 28, 11, 138, 211, 294, 4], [97, 13, 6, 28, 11, 138, 211, 8], [34, 929, 5, 18, 10, 43, 1361], [314, 262, 7, 14, 37, 74, 5, 1284, 2417], [360, 18, 5, 15, 16, 65, 6, 5, 68, 6, 13, 28, 11, 138, 211, 4], [148, 5, 7, 14, 11, 2418, 2419, 5, 7, 30, 142, 248, 4], [53, 6, 33, 1362], [6, 13, 82, 5, 62, 29, 347, 5, 74, 10, 17, 6, 94], [62, 11, 347, 5, 7, 14, 44, 51, 4], [7, 14, 105, 51], [53, 6], [6, 13, 82], [31, 32], [15, 6, 50, 9, 19, 377, 79, 26, 8], [42, 7, 15], [17, 15, 6, 50, 9, 38, 8], [7, 30, 15, 18, 5, 17, 15, 6, 50, 9, 38, 8], [36, 103, 35, 38, 907, 4], [21, 5, 7, 14, 351, 6, 38, 214, 7, 15, 16, 150, 197, 4], [7, 37, 9, 38, 907, 4], [36, 38, 11, 216, 104, 5, 38, 546], [30, 6, 665, 230, 26, 38, 8], [7, 50, 9, 2420, 6, 9, 38, 4], [21, 5, 47, 5, 51, 4], [7, 27, 48, 6, 63, 4], [86, 20], [20, 20], [31, 32], [12, 629, 23, 24, 902, 136, 124, 711, 4], [7, 19, 9, 88, 64, 132, 296, 4], [7, 15, 16, 70, 17, 7, 50, 9, 186, 4], [932, 79, 12, 232], [12, 629, 23, 24, 237, 136, 676, 500, 296, 4], [25, 10, 11, 310, 1363, 4], [42, 5, 74, 232, 10, 43, 825, 22, 18, 10, 43, 2421, 9, 26, 4], [25, 10, 41, 5, 18, 10, 11, 41, 232, 4], [45, 5, 103, 186, 159, 110, 25, 151, 4], [21, 5, 48, 6, 55, 151, 4], [21, 20, 20], [204], [31, 32], [7, 463, 11, 1275, 23, 11, 210, 58, 305, 9, 40, 11, 185, 67, 2422, 22, 633, 25, 7, 338, 4], [7, 19, 105, 374, 25, 12, 235, 10, 289, 16, 43, 41, 4], [12, 7, 455, 474, 10, 25, 102, 10, 2423, 121, 26, 4], [28, 723, 5, 55, 175, 1085, 5, 7, 66, 25, 12, 1364, 13, 44, 41, 22, 816, 4], [7, 63, 2424, 24, 530, 1364, 4, 102, 146, 25, 102, 154, 289, 2425, 4], [360, 18, 5, 15, 16, 142, 18, 5, 12, 2426, 119, 15, 16, 50, 9, 2427, 943, 4], [45, 5, 52, 57, 5, 335, 11, 185, 352, 4], [86], [36, 196, 296, 157, 69, 75, 4], [20, 20], [31, 32], [27, 363, 949, 28, 2428, 838, 2429, 8], [17, 10, 12, 950, 23, 29, 193, 98, 8], [496, 363, 949, 19, 1166, 2430, 690, 9, 1149, 690, 4], [74, 2431, 2432, 702, 10, 16, 2433, 5, 18, 10, 11, 2434, 523, 2435, 4], [1143, 5, 470, 5, 12, 2436, 10, 43, 519, 5, 152, 4], [7, 70, 11, 1365, 5, 98, 28, 531, 5, 7, 388, 49, 14, 142, 25, 12, 950, 23, 1365, 28, 12, 574, 4], [7, 66, 11, 602, 2437, 5, 214, 12, 950, 23, 951, 10, 107, 859, 68, 882, 2438, 5, 22, 12, 1282, 28, 951, 10, 41, 4], [7, 88, 18, 10, 118, 109, 104, 2439, 28, 310, 838, 4], [71, 98, 12, 949, 13, 301, 5, 951, 2440, 37, 78, 10, 11, 2441, 125, 2442, 1334, 23, 2443, 2444, 2445, 9, 2446], [515, 178, 96, 19, 104, 170, 4], [28, 723, 5, 12, 2447, 93, 16, 276, 28, 12, 2448, 5, 83], [25, 10, 16, 152, 5, 98, 7, 14, 2449, 2450, 352, 4], [83, 5, 21, 5, 48, 6, 4], [20, 20], [31, 32], [13, 6, 28, 207, 380, 8], [34, 5, 7, 339, 995, 28, 12, 147, 478, 456, 462, 4], [21, 5, 24, 164, 382, 404, 9, 19, 338, 12, 265, 4], [43, 2451, 5, 35, 172, 18, 4], [18, 10, 41], [68, 140, 457, 84, 149, 19, 374, 29, 382], [45, 5, 7, 27, 140, 63, 4], [86, 5, 48, 6, 63], [20, 20], [31, 32], [39, 10, 29, 482, 218, 33, 8], [7, 19, 148, 292, 4], [736, 5, 24, 1075, 5, 7, 143, 70, 153, 9, 35, 4], [93, 6, 19, 128, 1287, 2452, 8], [42, 5, 148], [736, 5, 18, 10, 312, 9, 172, 11, 482], [540, 433, 9, 11, 467, 50, 9, 2453, 26], [36, 35, 9, 12, 540, 433, 9, 2454, 12, 253, 5, 28, 12, 2455, 5, 18, 10, 12, 307, 470, 4], [105, 5, 2456, 6, 19, 575, 12, 222, 5, 6, 27, 40, 2457, 2458, 497, 279, 185, 1366, 4], [42, 5, 12, 529, 10, 43, 310, 4], [52, 57, 5, 116, 133, 5, 172, 11, 482], [45, 5, 52, 57, 4], [48, 6], [20, 20], [31, 32], [13, 6, 28, 207, 380, 8], [34, 5, 7, 14, 60, 164, 5, 17, 234, 8, 15, 6, 19, 132, 25, 2459, 57, 207, 380, 8], [42, 5, 7, 731, 12, 382, 23, 24, 164, 78, 4], [36, 35, 264, 33, 18, 4], [21], [13, 78, 128, 530, 118, 28, 207, 380, 5, 6, 30, 140], [34, 5, 7, 221, 218, 33, 18, 4], [21, 5, 264, 33, 18, 726, 4], [53, 6], [6, 13, 82], [31, 32], [19, 6, 171, 8], [7, 14, 11, 290, 5, 15, 16, 19, 9, 38, 5, 17, 64, 6, 8, 19, 6, 171, 8], [7, 364, 5, 1214, 316, 10, 274, 252, 4], [17, 93, 6, 38, 8], [7, 364, 543, 22, 876, 55, 383, 111, 4], [456, 952, 300, 43, 41], [23, 243, 5, 7, 14, 43, 546, 4], [47, 5, 1209, 41, 5, 147, 133, 4], [45, 5, 41, 5, 48, 6, 63, 4], [20, 20], [31, 32], [24, 379, 10, 2460, 5, 71, 12, 382, 91, 40, 633, 4], [17, 94, 7, 15, 8, 35, 9, 12, 712, 713, 22, 172, 18, 4], [21, 5, 7, 14, 218, 33, 18, 4], [45, 5, 172, 18, 56, 205], [21, 5, 7, 27, 35, 62, 205, 62, 583, 4], [39, 5, 19, 6, 633, 18, 8], [93, 16, 172, 18, 5, 18, 189, 19, 124, 2461, 160, 12, 265, 5, 22, 140, 122, 4], [34, 125, 10, 111, 8], [78, 13, 107, 2462, 2463, 731, 111, 4], [199, 5, 52, 56, 205], [21, 5, 7, 27, 40, 56, 205, 4], [45, 5, 106, 33, 6, 9, 52, 56, 4], [7, 14, 56], [21, 5, 7, 14, 60, 9, 405, 12, 379, 33, 6, 4], [47, 5, 21, 5, 48, 6, 4], [20, 20], [31, 32], [7, 1367, 543, 111, 4], [223, 5, 6, 13, 43, 41, 4], [23, 243], [17, 431, 19, 6, 171, 8], [24, 530, 105, 1367, 2464, 5, 876, 22, 191, 197, 4], [13, 925, 952, 252, 8], [925, 952, 13, 81, 252, 4], [25, 10, 41, 5, 6, 94, 19, 303, 4], [21, 5, 73, 105, 364, 544, 627], [96, 242], [17, 13, 6, 113, 98, 8], [73, 13, 118, 238, 4], [80, 367, 5, 2465, 124, 238, 81, 131, 4], [42, 5, 127, 293, 9, 381], [36, 6, 19, 9, 40, 51, 4], [45, 5, 21, 5, 39, 13, 6, 8], [7, 14, 118, 225, 159, 5, 7, 463, 11, 158, 23, 237, 4], [45, 5, 36, 6, 1148, 5, 7, 744, 901, 6, 4], [86, 20], [20, 20], [31, 32], [7, 14, 2466, 1128, 1368, 4], [108, 883, 5, 16, 41, 33, 29, 692], [21], [17, 13, 6, 113, 98, 8], [73, 13, 535, 742, 1369, 5, 113, 18, 2467, 5, 274, 41, 4], [223, 5, 6, 13, 44, 591, 4], [23, 243, 5, 103, 15, 18, 157, 69, 75, 4], [23, 243, 6, 30, 5, 27, 6, 15, 18, 8], [7, 27, 16, 5, 71, 7, 30, 594, 18, 396, 4], [21, 5, 83, 5, 157, 69, 75, 73, 120, 742, 1369, 110, 4], [73, 105, 364, 544, 627, 5, 43, 252], [47, 5, 42, 5, 43, 515], [274, 252, 5, 7, 19, 9, 38, 18, 4], [25, 10, 41, 5, 95, 38, 45, 4], [19, 6, 171, 8], [7, 14, 11, 290, 5, 7, 15, 16, 19, 9, 38, 4], [36, 6, 13, 16, 239, 8], [34, 5, 7, 150, 9, 1333, 101, 7, 14, 239, 4], [47, 5, 46, 1329], [53, 6], [6, 13, 82], [31, 32], [54, 6, 13, 429, 5, 103, 35, 9, 634, 5, 21, 8], [21], [103, 35, 48, 29, 2468, 2469, 4], [21, 5, 7, 50, 9, 35, 56, 22, 19, 11, 264, 4], [73, 30, 105, 2470, 160, 12, 297, 4], [21, 5, 47, 4], [84, 6, 15, 16, 35, 9, 863, 5, 27, 6, 35, 2471, 9, 634, 8], [34, 5, 80, 12, 307, 2472, 5, 12, 69, 75, 862, 57, 12, 605, 10, 12, 307, 4], [39, 384, 129, 18, 100, 9, 59, 9, 634, 8], [104, 121, 859, 2473], [10, 18, 44, 408, 8], [42, 5, 214, 729, 10, 28, 12, 2474, 23, 2475, 5, 634, 10, 28, 12, 2476], [36, 103, 35, 9, 863, 175, 5, 36, 35, 9, 634, 4], [23, 243, 5, 6, 30, 87, 132, 160, 12, 297, 4], [47, 5, 48, 6, 36, 4], [86, 5, 20], [20, 20], [31, 32], [7, 633, 12, 382, 25, 7, 338, 334, 4], [96, 8, 153, 93, 6, 172, 18, 8], [7, 154, 1208, 160, 24, 530, 5, 22, 102, 2477, 18, 56, 9, 26, 111, 4], [25, 10, 224, 4], [42, 5, 7, 105, 146, 53, 6, 9, 74, 714, 4], [224, 5, 7, 30, 35, 164, 4], [42, 5, 51], [47, 5, 48, 6, 63, 4], [86, 20], [20, 20], [31, 32], [54, 27, 6, 52, 164, 8], [54, 27, 6, 32, 12, 1293, 5, 54, 27, 7, 52, 56, 8], [47, 5, 1362, 9, 391, 584], [47, 5, 67, 6], [7, 67, 6, 72], [47, 5, 7, 14, 44, 51, 4], [25, 10, 41, 5, 20, 20, 4], [20, 20], [31, 32], [13, 6, 362, 56, 205, 8], [34, 5, 73, 13, 2478, 877], [2479, 178], [23, 243, 5, 47], [84, 6, 13, 362, 9, 61, 32, 5, 7, 14, 336, 9, 35, 56, 4], [73, 19, 9, 106, 11, 384, 69, 75, 5, 7, 27, 230, 6, 52, 56, 54, 6, 52, 56, 4], [21, 5, 36, 7, 14, 206, 33, 6, 4], [21, 5, 7, 27, 708, 6, 63, 4], [86, 20], [20, 20], [31, 32], [6, 264, 674, 111, 4], [34, 5, 7, 14, 176, 5, 7, 66, 43, 281, 5, 24, 1139, 13, 16, 405, 4], [45, 5, 25, 10, 41, 5, 13, 6, 60, 164, 8], [7, 19, 95, 403, 164], [45, 5, 21, 5, 7, 95, 221, 9, 12, 725, 22, 403, 336, 33, 77, 4], [21, 5, 147, 133, 5, 7, 27, 40, 336, 9, 35, 9, 12, 265, 63, 4], [45, 5, 41, 5, 52, 57, 4], [21, 20, 20], [20, 20], [31, 32], [7, 19, 9, 147, 133, 74, 381], [21, 5, 103, 147, 110, 4], [21, 5, 83, 4], [93, 6, 2480, 28, 77, 111, 8], [34, 5, 111, 7, 468, 2481, 24, 1370, 5, 18, 94, 40, 122, 4], [45, 5, 25, 10, 453, 41, 4], [7, 105, 88, 25, 78, 10, 11, 216, 104, 69, 75, 9, 854, 4], [52, 57, 122], [86, 20], [20, 20], [31, 32], [7, 50, 9, 38, 953, 839], [17, 304, 23, 953, 839], [2482, 2483, 5, 562, 50, 9, 38], [83, 5, 21, 5, 7, 27, 87, 6, 173, 63, 4], [21, 47], [206, 33, 26], [45, 5, 7, 118, 50, 9, 38, 481, 5, 7, 27, 35, 479, 63, 4], [7, 27, 87, 18, 33, 6, 63, 5, 407, 18, 56, 79, 12, 953, 1043, 4], [36, 7, 27, 106, 33, 6, 4], [47, 5, 86, 4], [36, 7, 27, 35, 9, 77, 9, 172, 6, 5, 103, 35, 9, 12, 635, 9, 147, 5, 22, 36, 35, 479, 110, 4], [21, 5, 48, 6, 63, 28, 12, 635, 4], [45, 5, 21, 5, 48, 6, 63, 4], [20, 20], [31, 32], [12, 232, 136, 16, 52, 139, 4], [97, 8, 93, 102, 526, 61, 358, 8], [102, 107, 146, 25, 102, 337, 35, 9, 77, 57, 699, 4, 7, 143, 70, 97, 74, 77, 337, 52, 4], [36, 106, 5, 291, 80, 95, 356, 4], [71, 102, 10, 162, 356], [25, 189, 40, 132, 431, 111, 4], [21, 5, 73, 95, 633, 49, 14, 4, 102, 107, 2484, 64, 238, 22, 2485, 9, 35, 9, 77, 4], [47, 5, 102, 10, 72, 117, 5, 22, 533, 923, 10, 16, 41, 4], [42, 5, 73, 13, 60, 9, 77, 22, 48, 6, 63, 4], [86, 20], [20, 20], [31, 32], [7, 27, 19, 11, 77, 152, 280, 4], [223, 5, 51], [103, 35, 9, 12, 635, 110, 4], [21, 5, 103, 48, 6, 28, 12, 635, 4], [21, 5, 6, 35, 9, 12, 635, 175, 5, 7, 27, 52, 63, 4], [21, 5, 48, 6, 63, 4], [20, 20], [20, 20], [31, 32], [12, 741, 1175, 608, 78, 10, 438, 74, 174, 4], [36, 7, 747, 61, 343, 54, 7, 221, 240, 111, 4], [71, 7, 15, 16, 19, 61, 343, 4], [148, 5, 84, 18, 96, 2486, 5, 7, 35, 9, 12, 254, 9, 87, 61, 343, 5, 22, 36, 406, 6, 126, 4], [45, 5, 21, 5, 53, 6, 4], [6, 13, 82], [31, 32], [19, 6, 171, 8], [7, 14, 11, 76, 5, 7, 15, 16, 150, 9, 38, 5, 17, 64, 6, 8], [7, 19, 171], [21, 5, 19, 6, 124, 546, 8], [7, 14, 546], [25, 10, 41, 5, 7, 27, 105, 1174, 679, 22, 1366, 4], [21, 5, 11, 158, 23, 279], [21], [67, 6], [6, 13, 44, 948], [53, 6], [6, 13, 82], [31, 32], [74, 741, 404, 9, 40, 60, 9, 438, 4], [42, 5, 12, 1210, 10, 546, 23, 2487, 2488], [7, 15, 16, 37, 438, 5, 22, 7, 15, 16, 19, 61, 343, 139, 4], [18, 129, 16, 247, 5, 7, 27, 406, 6, 126, 79, 61, 343, 4], [86, 5, 53, 6], [6, 13, 82], [31, 32], [7, 19, 11, 2489, 111, 4], [17, 234, 9, 6, 5, 19, 11, 693, 8], [34, 5, 7, 14, 72, 245, 9, 59, 126, 4], [21, 5, 7, 14, 72, 245, 111, 5, 7, 66, 43, 281, 4], [35, 9, 2490, 245, 74, 381, 4], [42, 5, 6, 468, 903, 126, 356, 4], [36, 6, 94, 15, 29, 159, 4], [21, 5, 7, 14, 28, 77, 4], [20, 20], [20, 20], [31, 32], [7, 734, 24, 596, 296, 4], [97, 5, 17, 234, 8], [7, 2491, 1260, 24, 596, 54, 7, 154, 535, 4], [6, 900, 1186, 126, 12, 2492, 22, 36, 1363, 79, 18, 4], [21, 5, 53, 6, 33, 29, 480, 4], [6, 13, 82], [31, 32], [7, 14, 60, 9, 77, 5, 17, 64, 6, 8], [7, 105, 161, 77, 4], [36, 103, 35, 9, 12, 254, 110, 8], [21, 5, 103, 35, 87, 132, 4], [21, 5, 7, 14, 206, 33, 6, 55, 12, 439, 4], [36, 6, 106, 33, 26, 33, 11, 467, 537, 5, 7, 27, 1200, 152, 280, 4], [48, 6, 63], [204], [31, 32], [39, 13, 6, 1165, 8], [7, 14, 547, 126, 5, 18, 27, 100, 11, 318, 4], [21, 5, 36, 7, 27, 106, 33, 6, 4], [21], [36, 6, 30, 405, 12, 379, 33, 26, 4], [21, 5, 48, 6, 63, 4], [204], [31, 32], [80, 437, 476, 4], [1194, 5, 78, 10, 61, 343, 28, 24, 1137, 4], [71, 7, 534, 9, 407, 61, 343, 4], [101, 6, 19, 161, 77, 5, 84, 18, 10, 118, 437, 5, 7, 27, 406, 6, 126, 4], [21, 5, 51], [47, 5, 48, 6, 63, 4], [86, 5, 20], [31, 32], [78, 13, 44, 228, 178, 28, 12, 254, 57, 1156, 4], [47, 5, 42, 5, 623, 10, 1348, 12, 1119, 4], [72, 228, 178, 5, 44, 2493], [47, 5, 2494, 5, 80, 41, 9, 87, 119, 4], [45, 5, 7, 19, 231, 403, 57, 12, 2495, 164, 4], [36, 52, 56, 205, 5, 83, 4], [21, 5, 106, 33, 26, 4], [48, 6, 63], [31, 32], [80, 437, 5, 80, 437, 4], [18, 129, 16, 247, 5, 7, 19, 61, 343, 5, 47, 4], [7, 27, 360, 9, 407, 61, 343, 4], [148, 5, 84, 18, 10, 118, 437, 5, 73, 27, 100, 11, 2496, 164, 4], [72, 1162], [47, 5, 71, 7, 422, 12, 438, 27, 616, 205, 4], [25, 10, 224], [47, 5, 36, 7, 27, 406, 6, 126, 63, 4], [47, 5, 51, 5, 48, 6, 63, 4], [20, 20], [31, 32], [15, 6, 35, 164, 475, 101, 77, 8], [42, 5, 103, 35, 56, 110, 8], [21, 5, 48, 6, 55, 12, 439, 63, 4], [86, 5, 20], [31, 32], [7, 66, 11, 2497], [17, 234, 5, 10, 18, 25, 7, 19, 171, 72, 228, 2498, 294, 8], [7, 15, 16, 70, 5, 24, 2499, 13, 274, 2500, 4], [6, 87, 173, 536, 22, 108, 104, 536, 4], [30, 7, 22, 2501, 8], [34, 5, 1368, 10, 16, 1154, 5, 108, 1236, 23, 536, 4], [21, 5, 53, 6, 33, 29, 480, 4], [47, 5, 6, 13, 82], [31, 32], [101, 77, 111, 5, 30, 6, 100, 11, 834, 33, 26, 8], [42, 5, 13, 6, 60, 9, 12, 193, 740, 8], [42, 5, 7, 14, 60, 9, 87, 132, 217, 4], [17, 13, 6, 60, 9, 87, 8], [7, 50, 9, 87, 11, 346, 2502, 33, 24, 347, 4], [15, 6, 150, 26, 9, 614, 6, 8], [34, 5, 7, 30, 35, 160, 217, 4], [21, 5, 36, 6, 52, 9, 26, 101, 77, 4], [21, 5, 51, 5, 83], [7, 27, 48, 6, 63, 4], [20, 20], [31, 32], [17, 69, 75, 13, 6, 60, 164, 111, 8], [581, 2503], [7, 14, 239], [7, 27, 87, 197, 63, 22, 315, 18, 9, 6, 4], [7, 50, 9, 38, 657, 4], [83, 5, 21, 5, 36, 15, 6, 37, 1371, 8], [7, 67, 1371], [83, 5, 21, 5, 7, 70, 4], [36, 7, 27, 106, 33, 6, 9, 87, 26, 197, 56, 4], [83, 5, 34, 177, 5, 106, 33, 26, 4], [21, 5, 48, 6, 63, 4], [20, 20], [31, 32], [49, 2504], [46, 5, 17, 234, 8], [148, 5, 7, 50, 9, 196, 9, 6, 4], [10, 29, 263, 2505, 8], [16, 139, 5, 80, 44, 133, 4], [18, 10, 21, 5, 220, 1356, 5, 261, 9, 2506, 4], [595, 44, 1354, 4], [7, 14, 105, 2507, 294, 5, 11, 158, 23, 119, 4], [36, 88, 64, 51, 119, 4], [21, 5, 51, 10, 195, 12, 256, 4], [93, 6, 866, 12, 2508, 9, 29, 210, 8], [34, 5, 80, 1372, 4], [97, 13, 6, 1372, 8], [34, 929, 5, 95, 15, 16, 50, 9, 196, 9, 49, 14], [18, 129, 16, 247, 5, 18, 27, 40, 176, 84, 12, 741, 10, 575, 4], [34, 5, 7, 15, 16, 19, 69, 75, 9, 100, 603, 23, 49, 14, 5, 7, 14, 43, 429, 4], [47, 5, 6, 622, 88, 28, 29, 894, 5, 97, 143, 6, 52, 9, 26, 8], [34, 5, 102, 93, 16, 50, 9, 251, 508, 5, 7, 27, 16, 196, 9, 49, 14, 4], [17, 93, 102, 15, 625, 8], [194, 5, 190, 5, 80, 81, 2509, 4], [10, 102, 2510, 126, 79, 191, 1373, 8], [7, 93, 16, 35, 9, 172, 371, 190, 5, 102, 2511, 9, 172, 11, 549, 4], [36, 97, 13, 6, 179, 8], [15, 16, 65, 6], [25, 189, 40, 148, 9, 40, 179, 64, 5, 25, 10, 5, 260, 757, 50, 9, 2512], [34, 5, 18, 10, 43, 179], [21, 5, 7, 143, 251, 17, 1373, 13, 488, 4], [1374, 13, 2513], [84, 11, 190, 10, 44, 441, 5, 18, 27, 16, 2514, 12, 2515, 23, 11, 549, 4], [39, 41, 10, 11, 549, 22, 11, 549, 5, 97, 13, 6, 218, 33, 11, 190, 8], [83, 5, 6, 30, 168, 9, 172, 11, 549, 110, 4], [7, 19, 11, 158, 23, 285, 258, 4], [25, 10, 95, 176, 5, 172, 11, 2516, 549, 5, 110], [34, 5, 7, 19, 9, 88, 64, 456, 1052, 1374, 33, 6, 4], [83], [17, 13, 6, 113, 8], [7, 14, 279, 4], [6, 13, 279, 250, 131, 4], [42, 5, 7, 37, 279, 43, 109, 4], [220, 9, 67, 26, 5, 7, 67, 9, 220], [893, 74, 10, 24, 2517], [39, 10, 29, 263, 60, 8], [7, 161, 18, 152, 280, 5, 71, 7, 143, 88, 12, 620, 10, 43, 41, 4], [223, 5, 25, 10, 224, 5, 7, 118, 19, 11, 158, 731, 9, 186, 4], [71, 7, 19, 9, 1358, 2518, 18, 5, 18, 10, 43, 312], [71, 29, 2519, 1033, 136, 124, 1198, 5, 18, 10, 43, 591, 4], [97, 93, 16, 6, 186, 705, 8], [47, 5, 7, 14, 2520, 5, 15, 16, 50, 9, 186], [47, 5, 52, 57, 5, 80, 43, 298, 4], [21, 5, 36, 7, 14, 60, 9, 186, 12, 434, 4], [45, 36, 5, 41, 1160], [20, 20], [20, 20], [31, 32], [13, 6, 231, 246, 8], [42, 5, 7, 14, 246, 33, 6, 4], [39, 154, 29, 116, 294, 8], [453, 41], [25, 10, 41], [6, 19, 9, 1190, 104, 22, 38, 45, 4], [21, 5, 6, 116, 133, 5, 7, 14, 60, 9, 147, 4], [45, 5, 41, 5, 20, 4], [20, 20], [31, 32], [7, 1335, 703, 9, 35, 9, 729, 9, 48, 6, 4], [36, 97, 93, 16, 6, 52, 8], [71, 98, 78, 10, 34, 69, 75, 5, 12, 279, 2521, 10, 1212, 4], [194, 5, 289, 5, 7, 143, 19, 69, 75, 4], [301, 529, 4], [7, 19, 294, 124, 523, 11, 158, 23, 529, 22, 12, 263, 10, 43, 312, 4], [96, 1361], [52, 57, 5, 6, 30, 5, 100, 11, 270, 245, 4], [45, 5, 52, 57, 5, 7, 27, 425, 175, 5, 41, 151, 4], [41, 151], [31, 32], [46, 5, 90], [6, 13, 12, 192, 89, 28, 24, 302, 4], [47, 5, 7, 14, 43, 51, 4], [163, 6, 37, 9, 19, 377, 79, 26, 55, 151, 8], [7, 14, 2522, 5, 74, 10, 24, 1072, 4], [21], [83, 5, 7, 27, 48, 6, 63, 4], [86, 20], [20, 20], [31, 32], [15, 7, 264, 41, 8], [223, 5, 6, 13, 44, 948, 4], [53, 6], [6, 13, 82], [31, 32], [7, 14, 12, 2523, 1370, 111, 4], [52, 57, 5, 52, 57, 4], [84, 6, 91, 106, 33, 26, 5, 35, 56, 175, 4], [148, 5, 7, 14, 206, 33, 6, 9, 35, 56, 110, 4], [21, 5, 103, 35, 9, 12, 254, 4], [21, 5, 6, 30, 35, 87, 173, 481, 9, 38, 4], [7, 27, 48, 6, 63, 4], [20, 20], [31, 32], [13, 6, 328, 8], [95, 161, 5, 6, 13, 206, 33, 26, 55, 12, 439, 4], [21, 5, 71, 7, 19, 9, 436, 12, 237, 4], [47, 5, 148, 5, 15, 16, 333, 4], [36, 6, 35, 9, 12, 439, 22, 106, 33, 26, 4, 7, 27, 52, 9, 6, 101, 7, 340, 12, 237, 4], [21, 5, 48, 6, 63, 4], [20, 20], [31, 32], [7, 1375, 18, 160, 125, 2524, 4], [97, 8], [623, 1375, 5, 107, 7, 93, 16, 471], [10, 12, 232, 2525, 128, 1271, 57, 6, 8], [7, 747, 1103, 849, 5, 44, 145, 4], [6, 22, 12, 232, 1252], [360, 18, 5, 7, 15, 16, 50, 9, 35, 9, 77, 4], [15, 16, 40, 37, 74, 5, 6, 19, 9, 19, 2526, 4], [7, 19, 12, 359, 528, 5, 7, 143, 70, 84, 7, 30, 471, 4], [7, 30, 580, 471, 5, 7, 509, 28, 6, 4], [12, 232, 95, 146, 25, 102, 624, 9, 196, 9, 457, 58, 136, 162, 124, 4], [36, 6, 140, 49, 14, 97, 102, 337, 471], [36, 7, 27, 140, 49, 14, 63, 4], [45, 5, 41, 5, 52, 57, 4], [52, 57], [31, 32], [24, 891, 10, 654, 240, 23, 517, 5, 7, 744, 35, 218, 33, 6, 63, 4], [19, 6, 161, 29, 77, 8], [16, 139], [39, 384, 129, 18, 100, 9, 59, 473, 12, 77, 8], [7, 15, 16, 70, 39, 384, 18, 27, 100, 4], [7, 27, 106, 33, 6, 55, 12, 439, 101, 1147, 537, 4, 95, 52, 9, 26, 475, 4], [71, 7, 118, 19, 11, 384, 69, 75, 4], [148, 5, 7, 14, 206, 33, 6, 4], [35, 56, 175, 4], [7, 14, 206, 33, 6, 4], [7, 14, 60, 9, 12, 439, 4], [21, 5, 7, 14, 206, 33, 6, 4], [7, 27, 48, 6, 63, 4], [86, 20], [20, 20], [31, 32], [123, 187, 110, 8], [106, 11, 865], [21, 5, 17, 13, 6, 113, 8], [7, 87, 173, 481, 9, 38, 5, 7, 14, 43, 239, 4], [21, 5, 36, 7, 14, 206, 33, 6, 5, 48, 6, 63, 4], [86, 20], [20, 20], [31, 32], [153, 15, 6, 52, 127, 8], [7, 52, 127, 267], [134, 268, 8], [323, 268], [134, 193, 8], [344, 193], [153, 15, 6, 52, 127, 8], [267, 5, 323, 268, 5, 344, 193], [31, 32], [17, 10, 29, 284, 8], [7, 37, 9, 550], [17, 324, 23, 325, 15, 6, 37, 8], [7, 37, 483, 551, 4], [58, 10, 29, 366, 8], [24, 366, 10, 552, 553]]\n",
            "753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DQ2LVWbIVO9p",
        "colab": {}
      },
      "source": [
        "# Make each sentences has the same length\n",
        "# sentence_batch: input text list as vector (length is batch_size, 16)\n",
        "# pad_int: PAD, which is 0\n",
        "# return: padded sequences (batch_x or y) and the sequence of sentence length\n",
        "def padding_sentence(s_batch, pad_num):\n",
        "    padded_seqs = []\n",
        "    seq_lens = []\n",
        "    max_len = max([len(sentence) for sentence in s_batch]) # the length of the longest sentence\n",
        "    for sentence in s_batch:\n",
        "        # sentence + [0] * the number of lacking words\n",
        "        padded_seqs.append(sentence + [pad_num] * (max_len - len(sentence)))\n",
        "        # the length of sentence\n",
        "        seq_lens.append(len(sentence))\n",
        "    return padded_seqs, seq_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYJ13XyKBmYj",
        "colab_type": "text"
      },
      "source": [
        "# **First Model: Bidirectional LSTM Seq2Seq + Greedy + Luong Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS-cWEoRoA8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Chatbot_Greedy:\n",
        "    def __init__(self, size_layer, num_layers, embedded_size, \n",
        "                 ques_dict_size, ans_dict_size, learning_rate, \n",
        "                 batch_size, dropout = 0.5):\n",
        "        \n",
        "        # Define LSTM cell\n",
        "        def lstm_cell(size, reuse=False):\n",
        "            ## Basic RNN\n",
        "            #return tf.nn.rnn_cell.BasicRNNCell(size_layer,reuse=reuse)\n",
        "            ## LSTM\n",
        "            #return tf.nn.rnn_cell.LSTMCell(size_layer, initializer=tf.orthogonal_initializer(), reuse=reuse)\n",
        "            # LSTM + Dropout (fix over fitting)\n",
        "            cell = tf.nn.rnn_cell.LSTMCell(size, initializer=tf.orthogonal_initializer(), reuse=reuse)\n",
        "            return tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=1.0, output_keep_prob=1.0)\n",
        "        \n",
        "        # Define GRU cell\n",
        "        def gru_cell(size, reuse=False):\n",
        "            ## GRU is better when the size of data base is small\n",
        "            cell = tf.nn.rnn_cell.GRUCell(size, reuse=reuse)\n",
        "            return tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=1.0, output_keep_prob=1.0)\n",
        "        \n",
        "        # Attention\n",
        "        def attention(encoder_out, seq_len, reuse=False):\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units = size_layer, \n",
        "                                                                    memory = encoder_out,\n",
        "                                                                    memory_sequence_length = seq_len)\n",
        "            #attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units = size_layer, \n",
        "            #                                                        memory = encoder_out,\n",
        "            #                                                        memory_sequence_length = seq_len)\n",
        "            return tf.contrib.seq2seq.AttentionWrapper(\n",
        "            cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell(size_layer, reuse) for _ in range(num_layers)]), \n",
        "                attention_mechanism = attention_mechanism,\n",
        "                attention_layer_size = size_layer)\n",
        "        \n",
        "        ########################################################################\n",
        "        self.X = tf.placeholder(tf.int32, [None, None]) # Input 1, parameter(type+[shape]), return Tensor\n",
        "        self.Y = tf.placeholder(tf.int32, [None, None]) # Input 2, target\n",
        "        # Computes number of nonzero elements across dimensions of a tensor\n",
        "        self.X_length = tf.count_nonzero(self.X, 1, dtype=tf.int32) \n",
        "        self.Y_length = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
        "        batch_size = tf.shape(self.X)[0]\n",
        "        \n",
        "        ## encoder\n",
        "        # define encoder embedding (basic), full connected\n",
        "        encoder_embeddings = tf.Variable(tf.random_uniform([ques_dict_size, embedded_size], -1, 1))\n",
        "        # dropout to prevent the overfitting\n",
        "        encoder_embeddings = tf.nn.dropout(encoder_embeddings, keep_prob = 1)\n",
        "        \n",
        "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
        "        # GRU cell composed sequentially of multiple simple cells.\n",
        "        encoder_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell(size_layer) for _ in range(num_layers)])\n",
        "                \n",
        "        # Bidirection lstm\n",
        "        for n in range(num_layers):\n",
        "            (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw = lstm_cell(size_layer // 2),\n",
        "                cell_bw = lstm_cell(size_layer // 2),\n",
        "                inputs = encoder_embedded,\n",
        "                sequence_length = self.X_length,\n",
        "                dtype = tf.float32,\n",
        "                scope = 'bidirectional_rnn_%d'%(n))\n",
        "            encoder_embedded = tf.concat((out_fw, out_bw), 2)\n",
        "        bi_state_c = tf.concat((state_fw.c, state_bw.c), -1)\n",
        "        bi_state_h = tf.concat((state_fw.h, state_bw.h), -1)\n",
        "        bi_lstm_state = tf.nn.rnn_cell.LSTMStateTuple(c=bi_state_c, h=bi_state_h)\n",
        "        self.encoder_state = tuple([bi_lstm_state] * num_layers)\n",
        "        \n",
        "        # Creates a recurrent neural network specified by GRUCell\n",
        "        self.encoder_out, encoder_state = tf.nn.dynamic_rnn(cell = encoder_cells, \n",
        "                                                                 inputs = encoder_embedded, \n",
        "                                                                 sequence_length = self.X_length,\n",
        "                                                                 dtype = tf.float32)\n",
        "        \n",
        "        self.encoder_state = tuple(self.encoder_state[-1] for _ in range(num_layers))\n",
        "        \n",
        "        \n",
        "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
        "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
        "        \n",
        "        ## decoder\n",
        "        # denfine decoder embedding\n",
        "        decoder_embeddings = tf.Variable(tf.random_uniform([ans_dict_size, embedded_size], -1, 1))\n",
        "        decoder_cell = attention(self.encoder_out, self.X_length) # Attention\n",
        "        dense_layer = tf.layers.Dense(ans_dict_size)\n",
        "        \n",
        "        ## greedy algorithm\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
        "                inputs = tf.nn.embedding_lookup(decoder_embeddings, decoder_input),\n",
        "                sequence_length = self.Y_length,\n",
        "                time_major = False)\n",
        "        \n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                cell = decoder_cell,\n",
        "                helper = training_helper,\n",
        "                initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=self.encoder_state),\n",
        "                output_layer = dense_layer)\n",
        "        \n",
        "        training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                decoder = training_decoder,\n",
        "                impute_finished = True,\n",
        "                maximum_iterations = tf.reduce_max(self.Y_length))\n",
        "        # logits for calculate loss\n",
        "        self.training_logits = training_decoder_output.rnn_output\n",
        "        \n",
        "        predict_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
        "                embedding = decoder_embeddings,\n",
        "                start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
        "                end_token = EOS)\n",
        "        \n",
        "        predict_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                cell = decoder_cell,\n",
        "                helper = predict_helper,\n",
        "                initial_state = decoder_cell.zero_state(batch_size, tf.float32).clone(cell_state=self.encoder_state),\n",
        "                output_layer = dense_layer)\n",
        "        \n",
        "        predict_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                decoder = predict_decoder,\n",
        "                impute_finished = True,\n",
        "                maximum_iterations = 2 * tf.reduce_max(self.X_length))\n",
        "        # predict_ids for prediction\n",
        "        self.predict_ids = predict_decoder_output.sample_id\n",
        "        \n",
        "        ###########################################################\n",
        "        masks = tf.sequence_mask(self.Y_length, tf.reduce_max(self.Y_length), dtype=tf.float32)\n",
        "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
        "                                                     targets = self.Y,\n",
        "                                                     weights = masks)\n",
        "        # adam optimizer and minimize the cost to optimize the model\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "        y_t = tf.argmax(self.training_logits,axis=2)\n",
        "        y_t = tf.cast(y_t, tf.int32)\n",
        "        self.prediction = tf.boolean_mask(y_t, masks)\n",
        "        mask_label = tf.boolean_mask(self.Y, masks)\n",
        "        correct_pred = tf.equal(self.prediction, mask_label) # return A Tensor of type bool.\n",
        "        correct_index = tf.cast(correct_pred, tf.float32) # Casts a tensor to a new type.\n",
        "        self.accuracy = tf.reduce_mean(correct_index) # get the average value at the axis\n",
        "        tf.summary.scalar(\"loss\",self.cost)\n",
        "        tf.summary.scalar(\"accuracy\",self.accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8Hgv5J_-ta4",
        "colab_type": "text"
      },
      "source": [
        "# ** Second Model (Optimization): 2 Bidirection GRU + Multiattention + Beam Search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7sgf-Or-V3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.util import nest\n",
        "\n",
        "class MultiAtt(tf.nn.rnn_cell.MultiRNNCell):\n",
        "    def __init__(self, attention_cell, cells, new_attention_using=True):\n",
        "        cells = [attention_cell] + cells\n",
        "        self.new_attention_using = new_attention_using\n",
        "        super(MultiAtt, self).__init__(\n",
        "            cells, state_is_tuple=True)\n",
        "\n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        \"\"\"Run the cell with bottom layer's attention copied to all upper layers.\"\"\"\n",
        "        if not nest.is_sequence(state):\n",
        "            raise ValueError(\n",
        "                \"Expected state to be a tuple of length %d, but received: %s\"\n",
        "                % (len(self.state_size), state))\n",
        "\n",
        "        with tf.variable_scope(scope or \"multi_rnn_cell\"):\n",
        "            new_states = []\n",
        "\n",
        "            with tf.variable_scope(\"cell_0_attention\"):\n",
        "                attention_cell = self._cells[0]\n",
        "                attention_state = state[0]\n",
        "                current_inp, new_attention_state = attention_cell(\n",
        "                    inputs, attention_state)\n",
        "                new_states.append(new_attention_state)\n",
        "\n",
        "            for i in range(1, len(self._cells)):\n",
        "                with tf.variable_scope(\"cell_%d\" % i):\n",
        "                    cell = self._cells[i]\n",
        "                    current_inp = state[i]\n",
        "\n",
        "                    if self.new_attention_using:\n",
        "                        current_inp = tf.concat(\n",
        "                            [current_inp, new_attention_state.attention], -1)\n",
        "                    else:\n",
        "                        current_inp = tf.concat(\n",
        "                            [current_inp, attention_state.attention], -1)\n",
        "\n",
        "                    current_inp, new_state = cell(current_inp, current_state)\n",
        "                    new_states.append(new_state)\n",
        "\n",
        "        return current_inp, tuple(new_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCYlNMAQul6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Chatbot_beam:\n",
        "    def __init__(self, size_layer, num_layers, embedded_size,\n",
        "                 ques_dict_size, ans_dict_size, learning_rate, beam_width = 5):\n",
        "        \n",
        "        def cells(size,reuse=False):\n",
        "            cell = tf.nn.rnn_cell.GRUCell(size, reuse=reuse)\n",
        "            return tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=1.0, output_keep_prob=1.0)\n",
        "        \n",
        "        self.X = tf.placeholder(tf.int32, [None, None])\n",
        "        self.Y = tf.placeholder(tf.int32, [None, None])\n",
        "        self.X_length = tf.count_nonzero(self.X, 1, dtype=tf.int32)\n",
        "        self.Y_length = tf.count_nonzero(self.Y, 1, dtype=tf.int32)\n",
        "        batch_size = tf.shape(self.X)[0]\n",
        "        \n",
        "        encoder_embeddings = tf.Variable(tf.random_uniform([ques_dict_size, embedded_size], -1, 1))\n",
        "        encoder_embeddings = tf.nn.dropout(encoder_embeddings, keep_prob = 1)\n",
        "        encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, self.X)\n",
        "        \n",
        "        gru_layer = 3\n",
        "\n",
        "        # 2 biGRU layers\n",
        "        for n in range(2):\n",
        "            (out_fw, out_bw), (state_fw, state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
        "                cell_fw = cells(size_layer),\n",
        "                cell_bw = cells(size_layer),\n",
        "                inputs = encoder_embedded,\n",
        "                sequence_length = self.X_length,\n",
        "                dtype = tf.float32,\n",
        "                scope = 'bidirectional_rnn_%d'%(n))\n",
        "            encoder_embedded = tf.concat((out_fw, out_bw), 2)\n",
        "        gru_cells = tf.nn.rnn_cell.MultiRNNCell([cells(size_layer) for _ in range(gru_layer)])\n",
        "        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
        "                gru_cells,\n",
        "                encoder_embedded,\n",
        "                dtype=tf.float32,\n",
        "                sequence_length=self.X_length)\n",
        "        \n",
        "        encoder_state = (state_bw,) + (\n",
        "                (encoder_state,) if gru_layer == 1 else encoder_state)\n",
        "        \n",
        "        main = tf.strided_slice(self.Y, [0, 0], [batch_size, -1], [1, 1])\n",
        "        \n",
        "        decoder_embeddings = tf.Variable(tf.random_uniform([ans_dict_size, embedded_size], -1, 1))\n",
        "        decoder_input = tf.concat([tf.fill([batch_size, 1], GO), main], 1)\n",
        "        decoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, decoder_input)\n",
        "        \n",
        "        decoder_cells = []\n",
        "        for n in range(num_layers):\n",
        "            cell = cells(size_layer)\n",
        "            decoder_cells.append(cell)\n",
        "        attention_cell = decoder_cells.pop(0)\n",
        "        to_dense = tf.layers.Dense(ans_dict_size)\n",
        "        \n",
        "        with tf.variable_scope('decode'):\n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                num_units = size_layer, \n",
        "                memory = encoder_outputs,\n",
        "                memory_sequence_length = self.X_length)\n",
        "            attentionNew_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                cell = attention_cell,\n",
        "                attention_mechanism = attention_mechanism,\n",
        "                attention_layer_size = None,\n",
        "                alignment_history = True,\n",
        "                output_attention = False)\n",
        "            multi_attention_cell = MultiAtt(attentionNew_cell, decoder_cells)\n",
        "            \n",
        "            self.initial_state = tuple(\n",
        "                zs.clone(cell_state=es)\n",
        "                if isinstance(zs, tf.contrib.seq2seq.AttentionWrapperState) else es\n",
        "                for zs, es in zip(\n",
        "                    multi_attention_cell.zero_state(batch_size, dtype=tf.float32), encoder_state))\n",
        "            \n",
        "            training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
        "                decoder_embedded,\n",
        "                self.Y_length,\n",
        "                time_major = False\n",
        "            )\n",
        "            training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
        "                cell = multi_attention_cell,\n",
        "                helper = training_helper,\n",
        "                initial_state = self.initial_state,\n",
        "                output_layer = to_dense)\n",
        "            training_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                decoder = training_decoder,\n",
        "                impute_finished = True,\n",
        "                maximum_iterations = tf.reduce_max(self.Y_length))\n",
        "            \n",
        "        with tf.variable_scope('decode', reuse=True):\n",
        "            encoder_out_tiled = tf.contrib.seq2seq.tile_batch(encoder_outputs, beam_width)\n",
        "            encoder_state_tiled = tf.contrib.seq2seq.tile_batch(encoder_state, beam_width)\n",
        "            X_length_tiled = tf.contrib.seq2seq.tile_batch(self.X_length, beam_width)\n",
        "            \n",
        "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
        "                num_units = size_layer, \n",
        "                memory = encoder_out_tiled,\n",
        "                memory_sequence_length = X_length_tiled)\n",
        "            attentionNew_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
        "                cell = attention_cell,\n",
        "                attention_mechanism = attention_mechanism,\n",
        "                attention_layer_size = None,\n",
        "                alignment_history = False,\n",
        "                output_attention = False)\n",
        "            multi_attention_cell = MultiAtt(attentionNew_cell, decoder_cells)\n",
        "            \n",
        "            self.initial_state = tuple(\n",
        "                zs.clone(cell_state=es)\n",
        "                if isinstance(zs, tf.contrib.seq2seq.AttentionWrapperState) else es\n",
        "                for zs, es in zip(\n",
        "                    multi_attention_cell.zero_state(batch_size * beam_width, dtype=tf.float32), encoder_state_tiled))\n",
        "            \n",
        "            predict_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "                cell = multi_attention_cell,\n",
        "                embedding = decoder_embeddings,\n",
        "                start_tokens = tf.tile(tf.constant([GO], dtype=tf.int32), [batch_size]),\n",
        "                end_token = EOS,\n",
        "                initial_state = self.initial_state,\n",
        "                beam_width = beam_width,\n",
        "                output_layer = to_dense,\n",
        "                length_penalty_weight = 0.0)\n",
        "            predict_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                decoder = predict_decoder,\n",
        "                impute_finished = False,\n",
        "                maximum_iterations = 2 * tf.reduce_max(self.X_length))\n",
        "            \n",
        "            self.training_logits = training_decoder_output.rnn_output\n",
        "            self.predict_ids = predict_decoder_output.predicted_ids[:, :, 0]\n",
        "            \n",
        "        masks = tf.sequence_mask(self.Y_length, tf.reduce_max(self.Y_length), dtype=tf.float32)\n",
        "        self.cost = tf.contrib.seq2seq.sequence_loss(logits = self.training_logits,\n",
        "                                                     targets = self.Y,\n",
        "                                                     weights = masks)\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)\n",
        "        \n",
        "        y_t = tf.argmax(self.training_logits,axis=2)\n",
        "        y_t = tf.cast(y_t, tf.int32)\n",
        "        self.prediction = tf.boolean_mask(y_t, masks)\n",
        "        mask_label = tf.boolean_mask(self.Y, masks)\n",
        "        correct_pred = tf.equal(self.prediction, mask_label)\n",
        "        correct_index = tf.cast(correct_pred, tf.float32)\n",
        "        self.accuracy = tf.reduce_mean(correct_index)\n",
        "        tf.summary.scalar(\"loss\",self.cost)\n",
        "        tf.summary.scalar(\"accuracy\",self.accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSMU9tBloA8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "size_layer = 256\n",
        "num_layers = 2\n",
        "embedded_size = 128\n",
        "learning_rate = 0.001\n",
        "batch_size = 16\n",
        "epoch = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7a565466-c71b-47d2-8324-9ea70441cd5a",
        "id": "bIXiddLVn-vQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# initialize the model\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "#First Model\n",
        "model = Chatbot_Greedy(size_layer, num_layers, embedded_size, len(dictionary_ques), \n",
        "                len(dictionary_ans), learning_rate, batch_size)\n",
        "#Second Model\n",
        "#model = Chatbot_beam(size_layer, num_layers, embedded_size, len(dictionary_ques), \n",
        "#                len(dictionary_ans), learning_rate, batch_size)\n",
        "\n",
        "# run\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sky3GdDSBxkU",
        "colab_type": "text"
      },
      "source": [
        "# **Training Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcLwr0C0oA9C",
        "colab_type": "code",
        "outputId": "cf030aeb-4eae-4c3b-abd4-f827cfd9d60f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        }
      },
      "source": [
        "merged_summary = tf.summary.merge_all()\n",
        "# Write the model into a file\n",
        "writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
        "\n",
        "for i in range(epoch):\n",
        "    avg_loss, avg_accuracy = 0, 0\n",
        "    for j in range(0, len(question_train), batch_size):\n",
        "        index = min(j+batch_size, len(question_train))\n",
        "        batch_input, seq_input = padding_sentence(X[j: index], PAD)\n",
        "        batch_target, seq_target = padding_sentence(Y[j: index], PAD)\n",
        "        \n",
        "        prediction, accuracy,loss, _, summary = sess.run([model.predict_ids, \n",
        "                                                model.accuracy, model.cost, model.optimizer, merged_summary], \n",
        "                                      feed_dict={model.X:batch_input,\n",
        "                                                model.Y:batch_target})\n",
        "        avg_loss += loss\n",
        "        avg_accuracy += accuracy\n",
        "        writer.add_summary(summary,j)\n",
        "    # Calculated the average loss and accuracy for each epoch\n",
        "    avg_loss /= (len(question_train) / batch_size)\n",
        "    avg_accuracy /= (len(question_train) / batch_size)\n",
        "    print('epoch: %d, avg loss: %f, avg accuracy: %f'%(i+1, avg_loss, avg_accuracy))\n",
        "    #30+16\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, avg loss: 5.189822, avg accuracy: 0.160449\n",
            "epoch: 2, avg loss: 4.322139, avg accuracy: 0.236345\n",
            "epoch: 3, avg loss: 3.941374, avg accuracy: 0.279842\n",
            "epoch: 4, avg loss: 3.677100, avg accuracy: 0.311542\n",
            "epoch: 5, avg loss: 3.457764, avg accuracy: 0.330802\n",
            "epoch: 6, avg loss: 3.245541, avg accuracy: 0.354192\n",
            "epoch: 7, avg loss: 3.053195, avg accuracy: 0.374060\n",
            "epoch: 8, avg loss: 2.869162, avg accuracy: 0.394869\n",
            "epoch: 9, avg loss: 2.663800, avg accuracy: 0.421280\n",
            "epoch: 10, avg loss: 2.472649, avg accuracy: 0.453201\n",
            "epoch: 11, avg loss: 2.272226, avg accuracy: 0.483445\n",
            "epoch: 12, avg loss: 2.104848, avg accuracy: 0.513559\n",
            "epoch: 13, avg loss: 1.982466, avg accuracy: 0.539791\n",
            "epoch: 14, avg loss: 1.698072, avg accuracy: 0.597925\n",
            "epoch: 15, avg loss: 1.514624, avg accuracy: 0.636178\n",
            "epoch: 16, avg loss: 1.408098, avg accuracy: 0.655531\n",
            "epoch: 17, avg loss: 1.227180, avg accuracy: 0.696003\n",
            "epoch: 18, avg loss: 1.102267, avg accuracy: 0.722094\n",
            "epoch: 19, avg loss: 1.035100, avg accuracy: 0.733834\n",
            "epoch: 20, avg loss: 0.919353, avg accuracy: 0.765235\n",
            "epoch: 21, avg loss: 0.788643, avg accuracy: 0.799084\n",
            "epoch: 22, avg loss: 0.716833, avg accuracy: 0.815607\n",
            "epoch: 23, avg loss: 0.632178, avg accuracy: 0.835976\n",
            "epoch: 24, avg loss: 0.553497, avg accuracy: 0.855617\n",
            "epoch: 25, avg loss: 0.490942, avg accuracy: 0.872319\n",
            "epoch: 26, avg loss: 0.438273, avg accuracy: 0.885227\n",
            "epoch: 27, avg loss: 0.380351, avg accuracy: 0.900950\n",
            "epoch: 28, avg loss: 0.362928, avg accuracy: 0.906744\n",
            "epoch: 29, avg loss: 0.336438, avg accuracy: 0.912771\n",
            "epoch: 30, avg loss: 0.294534, avg accuracy: 0.924500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJao6eSnFZN7",
        "colab_type": "text"
      },
      "source": [
        "# **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qbMusP9aQZy",
        "colab_type": "code",
        "outputId": "c1480bcd-2ded-4d16-d4e3-bd66a88c90ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "#saver.save(sess, 'my_test_model')\n",
        "\n",
        "saver.restore(sess, \"my_test_model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0905 12:52:40.063646 140399066265472 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EhWsp8BFd2v",
        "colab_type": "text"
      },
      "source": [
        "# **Visualize Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpQvgGElZnvs",
        "colab_type": "code",
        "outputId": "2b8b49de-94b8-4323-94a8-7682fe7a73c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-05 11:40:35--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.195.49.195, 52.22.235.225, 35.173.3.255, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.195.49.195|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13607069 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  12.98M  6.43MB/s    in 2.0s    \n",
            "\n",
            "2019-09-05 11:40:43 (6.43 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13607069/13607069]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8qxJlu1YlCU",
        "colab_type": "code",
        "outputId": "7b974dc7-2efa-487f-ea68-c9e568ad7edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Visualiazed\n",
        "LOG_DIR = 'logs'\n",
        "\n",
        "get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "\"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://5ab35353.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8Vz4DoMoA9G",
        "colab_type": "code",
        "outputId": "1b6ba513-ed85-418a-bea3-c95ec1614056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(len(batch_input)):\n",
        "    print('Conversation %d'%(i+1))\n",
        "    print('QUESTION:',' '.join([rev_dictionary_ques[n] for n in batch_input[i] if n not in [0,1,2,3]]))\n",
        "    print('PREDICTION:',' '.join([rev_dictionary_ans[n] for n in prediction[i] if n not in[0,1,2,3]]),'\\n')\n",
        "    print('EXPECTED ANSWER:',' '.join([rev_dictionary_ans[n] for n in batch_target[i] if n not in[0,1,2,3]]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conversation 1\n",
            "QUESTION: not do be stock , you are allowed but .\n",
            "PREDICTION: i am software laugh should . \n",
            "\n",
            "EXPECTED ANSWER: sadness , yes\n",
            "Conversation 2\n",
            "QUESTION: sadness , yes\n",
            "PREDICTION: see , at . \n",
            "\n",
            "EXPECTED ANSWER: i am get ? fast games .\n",
            "Conversation 3\n",
            "QUESTION: i am get ? fast games .\n",
            "PREDICTION: my the good . \n",
            "\n",
            "EXPECTED ANSWER: very , boyfriend on a eaters .\n",
            "Conversation 4\n",
            "QUESTION: very , boyfriend on a eaters .\n",
            "PREDICTION: bye , i am out on a would . \n",
            "\n",
            "EXPECTED ANSWER: i watch thank eating my ask .\n",
            "Conversation 5\n",
            "QUESTION: i watch thank eating my ask .\n",
            "PREDICTION: very , bye \n",
            "\n",
            "EXPECTED ANSWER: all\n",
            "Conversation 6\n",
            "QUESTION: all\n",
            "PREDICTION: why spent happy ask . \n",
            "\n",
            "EXPECTED ANSWER: ok even\n",
            "Conversation 7\n",
            "QUESTION: ok even\n",
            "PREDICTION: i ok do poker ? amazing to \n",
            "\n",
            "EXPECTED ANSWER: i later my i not do want ? like happy ask .\n",
            "Conversation 8\n",
            "QUESTION: i later my i not do want ? like happy ask .\n",
            "PREDICTION: you are is good \n",
            "\n",
            "EXPECTED ANSWER: hi monday to\n",
            "Conversation 9\n",
            "QUESTION: hi monday to\n",
            "PREDICTION: your a capable supports the is learning to \n",
            "\n",
            "EXPECTED ANSWER: very , dreams let ? which , saturday i studying no .\n",
            "Conversation 10\n",
            "QUESTION: very , dreams let ? which , saturday i studying no .\n",
            "PREDICTION: what not i later how work to \n",
            "\n",
            "EXPECTED ANSWER: come thank eating my ask .\n",
            "Conversation 11\n",
            "QUESTION: come thank eating my ask .\n",
            "PREDICTION: supermarket easily home \n",
            "\n",
            "EXPECTED ANSWER: very , i calm watching home .\n",
            "Conversation 12\n",
            "QUESTION: very , i calm watching home .\n",
            "PREDICTION: go i ok feel love ? a exams the good . \n",
            "\n",
            "EXPECTED ANSWER: bye , class you not do want ? no , you that do no .\n",
            "Conversation 13\n",
            "QUESTION: bye , class you not do want ? no , you that do no .\n",
            "PREDICTION: very , i am he out feel love , i so you hear i much ? you . \n",
            "\n",
            "EXPECTED ANSWER: ti the it do too good to\n",
            "Conversation 14\n",
            "QUESTION: ti the it do too good to\n",
            "PREDICTION: as me is something maybe \n",
            "\n",
            "EXPECTED ANSWER: he , come thank automobile .\n",
            "Conversation 15\n",
            "QUESTION: he , come thank automobile .\n",
            "PREDICTION: bye \n",
            "\n",
            "EXPECTED ANSWER: all\n",
            "Conversation 16\n",
            "QUESTION: all\n",
            "PREDICTION: why spent happy ask . \n",
            "\n",
            "EXPECTED ANSWER: i that no where an .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBY1TNjzB18H",
        "colab_type": "text"
      },
      "source": [
        "# **Test Model using test dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl_mcVuqoA9L",
        "colab_type": "code",
        "outputId": "d620d35b-85cd-4dbe-c89a-94580b097bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_input, seq_input = padding_sentence(X_test[:batch_size], PAD)\n",
        "batch_target, seq_target = padding_sentence(Y_test[:batch_size], PAD)\n",
        "prediction = sess.run(model.predict_ids, feed_dict={model.X:batch_input,model.X_length:seq_input})\n",
        "\n",
        "for i in range(len(batch_input)):\n",
        "    print('Conversation %d'%(i+1))\n",
        "    print('QUESTION:',' '.join([rev_dictionary_ques[n] for n in batch_input[i] if n not in [0,1,2,3]]))\n",
        "    print('PREDICTION:',' '.join([rev_dictionary_ans[n] for n in prediction[i] if n not in[0,1,2,3]]),'\\n')\n",
        "    print('EXPECTED ANSWER:',' '.join([rev_dictionary_ans[n] for n in batch_target[i] if n not in[0,1,2,3]]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Conversation 1\n",
            "QUESTION: hello\n",
            "PREDICTION: hello \n",
            "\n",
            "EXPECTED ANSWER: hello\n",
            "Conversation 2\n",
            "QUESTION: hello\n",
            "PREDICTION: hello \n",
            "\n",
            "EXPECTED ANSWER: have you eaten ?\n",
            "Conversation 3\n",
            "QUESTION: have you eaten ?\n",
            "PREDICTION: bye , have \n",
            "\n",
            "EXPECTED ANSWER: i am a robot , no need to eat\n",
            "Conversation 4\n",
            "QUESTION: i am a robot , no need to eat\n",
            "PREDICTION: what the for meet \n",
            "\n",
            "EXPECTED ANSWER: then are you not hungry ?\n",
            "Conversation 5\n",
            "QUESTION: then are you not hungry ?\n",
            "PREDICTION: my the today well called . \n",
            "\n",
            "EXPECTED ANSWER: i am not hungry , i just want to cook for you .\n",
            "Conversation 6\n",
            "QUESTION: i am not hungry , i just want to cook for you .\n",
            "PREDICTION: do something , school \n",
            "\n",
            "EXPECTED ANSWER: can you cook ?\n",
            "Conversation 7\n",
            "QUESTION: can you cook ?\n",
            "PREDICTION: i door not do or hard shaanxi , i am do course i not \n",
            "\n",
            "EXPECTED ANSWER: yes , i will wash vegetables , cut vegetables , stirfry .\n",
            "Conversation 8\n",
            "QUESTION: yes , i will wash vegetables , cut vegetables , stirfry .\n",
            "PREDICTION: see , i am told on know province will a person . i not do okay class you really a feel love . \n",
            "\n",
            "EXPECTED ANSWER: then you do it slowly , i will come back later .\n",
            "Conversation 9\n",
            "QUESTION: then you do it slowly , i will come back later .\n",
            "PREDICTION: eat not you later to \n",
            "\n",
            "EXPECTED ANSWER: when can you come back ?\n",
            "Conversation 10\n",
            "QUESTION: when can you come back ?\n",
            "PREDICTION: wednesday the is hat coming . \n",
            "\n",
            "EXPECTED ANSWER: 6 oclock in the afternoon\n",
            "Conversation 11\n",
            "QUESTION: 6 oclock in the afternoon\n",
            "PREDICTION: is cannot the is cannot afternoon . \n",
            "\n",
            "EXPECTED ANSWER: then i should have done it .\n",
            "Conversation 12\n",
            "QUESTION: then i should have done it .\n",
            "PREDICTION: all \n",
            "\n",
            "EXPECTED ANSWER: then you are doing very fast .\n",
            "Conversation 13\n",
            "QUESTION: then you are doing very fast .\n",
            "PREDICTION: you are do on even i later distribution \n",
            "\n",
            "EXPECTED ANSWER: of course , i am very powerful .\n",
            "Conversation 14\n",
            "QUESTION: of course , i am very powerful .\n",
            "PREDICTION: go i am hi game \n",
            "\n",
            "EXPECTED ANSWER: haha , see you that afternoon .\n",
            "Conversation 15\n",
            "QUESTION: haha , see you that afternoon .\n",
            "PREDICTION: i ok feelings , i am do i every . \n",
            "\n",
            "EXPECTED ANSWER: goodbye\n",
            "Conversation 16\n",
            "QUESTION: goodbye\n",
            "PREDICTION: bye , ame are do live end is forbidden . \n",
            "\n",
            "EXPECTED ANSWER: bye bye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Miz8YYDZB5xr",
        "colab_type": "text"
      },
      "source": [
        "# **User input interface**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0UtDTnErtaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def user_input(input_text):\n",
        "    sentence = [input_text]\n",
        "    # use the same rules for user input text\n",
        "    clean_text(sentence)\n",
        "    vec_sentence = vectorize(sentence,dictionary_ques)\n",
        "    batch_input, seq_input = padding_sentence(vec_sentence[:batch_size], PAD)\n",
        "    prediction = sess.run(model.predicting_ids, feed_dict={model.X:batch_input,model.X_length:seq_input})\n",
        "    \n",
        "    print('Sunday:',' '.join([rev_dictionary_ans[n] for n in prediction[0] if n not in[0,1,2,3]]),'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5smB30ZrjHTo",
        "colab_type": "code",
        "outputId": "5574f985-4ef3-4d77-848f-4cfab4610ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "# user input\n",
        "text = ''\n",
        "while text != 'bye':\n",
        "      text = input('me: ')\n",
        "      user_input(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "me: hi\n",
            "Sunday: hi \n",
            "\n",
            "me: what is your name?\n",
            "Sunday: my name is sunday \n",
            "\n",
            "me: How old are you?\n",
            "Sunday: i am 25 years old . \n",
            "\n",
            "me: How are you?\n",
            "Sunday: fine thanks . \n",
            "\n",
            "me: What are you doing?\n",
            "Sunday: i am writing old . \n",
            "\n",
            "me: I am missing you\n",
            "Sunday: hahaha , i am so happy to hear \n",
            "\n",
            "me: bye\n",
            "Sunday: conversation end \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D0qbQiuI5qU",
        "colab_type": "text"
      },
      "source": [
        "# **Connect the model with WeChat API**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCSWFvG_FbYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wechat_input(input_text):\n",
        "    sentence = [input_text]\n",
        "    # use the same rules for user input text\n",
        "    clean_text(sentence)\n",
        "    vec_sentence = vectorize(sentence,dictionary_ques)\n",
        "    batch_input, seq_input = padding_sentence(vec_sentence[:batch_size], PAD)\n",
        "    prediction = sess.run(model.predict_ids, feed_dict={model.X:batch_input,model.X_length:seq_input})\n",
        "    \n",
        "    return(' '.join([rev_dictionary_ans[n] for n in prediction[0] if n not in[0,1,2,3]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5fykD-P1vCe",
        "colab_type": "code",
        "outputId": "f46ab461-d6e2-4a76-c5b5-bf139bffc8eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "pip install itchat"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting itchat\n",
            "  Downloading https://files.pythonhosted.org/packages/57/99/20dde4bee645453d1453ae3757b49f24a5fd179ce6e391cf2542cfeac61c/itchat-1.3.10-py2.py3-none-any.whl\n",
            "Collecting pypng (from itchat)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz (649kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from itchat) (2.21.0)\n",
            "Collecting pyqrcode (from itchat)\n",
            "  Downloading https://files.pythonhosted.org/packages/37/61/f07226075c347897937d4086ef8e55f0a62ae535e28069884ac68d979316/PyQRCode-1.2.1.tar.gz\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->itchat) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->itchat) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->itchat) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->itchat) (1.24.3)\n",
            "Building wheels for collected packages: pypng, pyqrcode\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-cp36-none-any.whl size=67163 sha256=dbc29aa6b7bcc68ed2c0dd12a02add5652c7237ca514ebe8da71ef76a1cbcd5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/6b/ef/0493b536b6d4722c2ae9486691b1d49b922b9877922beeabb3\n",
            "  Building wheel for pyqrcode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyqrcode: filename=PyQRCode-1.2.1-cp36-none-any.whl size=36247 sha256=b11e69fd44598aa8346ee5580331223f95af8790b2b4b47bee4e2fcc2938c7f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/61/2f/a4f51b09473db5737db0f654ed10eb9a4ee01f83a7412de349\n",
            "Successfully built pypng pyqrcode\n",
            "Installing collected packages: pypng, pyqrcode, itchat\n",
            "Successfully installed itchat-1.3.10 pypng-0.0.20 pyqrcode-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAXmbntGDofX",
        "colab_type": "code",
        "outputId": "2c248d26-3e44-4900-89ea-66f17c64d281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import itchat\n",
        "\n",
        "@itchat.msg_register(itchat.content.TEXT)\n",
        "def text_reply(msg):\n",
        "    reply_ = wechat_input(msg.text)\n",
        "    return reply_\n",
        "\n",
        "itchat.logout()\n",
        "itchat.auto_login()\n",
        "itchat.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "█\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Getting uuid of QR code.\n",
            "I0905 12:55:10.522500 140399066265472 login.py:44] Getting uuid of QR code.\n",
            "Downloading QR code.\n",
            "I0905 12:55:11.011609 140399066265472 login.py:47] Downloading QR code.\n",
            "Please scan the QR code to log in.\n",
            "I0905 12:55:11.338150 140399066265472 login.py:50] Please scan the QR code to log in.\n",
            "Please press confirm on your phone.\n",
            "I0905 12:55:29.891390 140399066265472 login.py:60] Please press confirm on your phone.\n",
            "Loading the contact, this may take a little while.\n",
            "I0905 12:55:31.904147 140399066265472 login.py:70] Loading the contact, this may take a little while.\n",
            "Login successfully as 杨浩泽\n",
            "I0905 12:55:35.508076 140399066265472 login.py:80] Login successfully as 杨浩泽\n",
            "Start auto replying.\n",
            "I0905 12:55:35.513076 140399066265472 register.py:85] Start auto replying.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.UnimplementedError: 2 root error(s) found.\n",
            "  (0) Unimplemented: TensorArray has size zero, but element shape [?] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\n",
            "\t [[{{node decoder_1/TensorArrayStack_1/TensorArrayGatherV3}}]]\n",
            "  (1) Unimplemented: TensorArray has size zero, but element shape [?,128] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\n",
            "\t [[{{node bidirectional_rnn_0/bw/bw/TensorArrayStack/TensorArrayGatherV3}}]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/itchat/components/register.py\", line 60, in configured_reply\n",
            "    r = replyFn(msg)\n",
            "  File \"<ipython-input-74-220cca16944d>\", line 5, in text_reply\n",
            "    reply_ = wechat_input(msg.text)\n",
            "  File \"<ipython-input-72-e63b7913d9db>\", line 7, in wechat_input\n",
            "    prediction = sess.run(model.predict_ids, feed_dict={model.X:batch_input,model.X_length:seq_input})\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.UnimplementedError: 2 root error(s) found.\n",
            "  (0) Unimplemented: TensorArray has size zero, but element shape [?] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\n",
            "\t [[node decoder_1/TensorArrayStack_1/TensorArrayGatherV3 (defined at <ipython-input-63-bff4a1c4fd7e>:119) ]]\n",
            "  (1) Unimplemented: TensorArray has size zero, but element shape [?,128] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\n",
            "\t [[node bidirectional_rnn_0/bw/bw/TensorArrayStack/TensorArrayGatherV3 (defined at <ipython-input-63-bff4a1c4fd7e>:61) ]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "Original stack trace for 'decoder_1/TensorArrayStack_1/TensorArrayGatherV3':\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
            "    ioloop.IOLoop.instance().start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n",
            "    handler_func(fd_obj, events)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
            "    self._handle_recv()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
            "    self._run_callback(callback, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
            "    callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
            "    return self.dispatch_shell(stream, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
            "    handler(stream, idents, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
            "    user_expressions, allow_stdin)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
            "    interactivity=interactivity, compiler=compiler, result=result)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
            "    if self.run_code(code, result):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-67-4dbc035edcfb>\", line 5, in <module>\n",
            "    len(dictionary_ans), learning_rate, batch_size)\n",
            "  File \"<ipython-input-63-bff4a1c4fd7e>\", line 119, in __init__\n",
            "    maximum_iterations = 2 * tf.reduce_max(self.X_length))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in dynamic_decode\n",
            "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 515, in map_structure\n",
            "    structure[0], [func(*x) for x in entries],\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 515, in <listcomp>\n",
            "    structure[0], [func(*x) for x in entries],\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in <lambda>\n",
            "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1205, in stack\n",
            "    return self._implementation.stack(name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 309, in stack\n",
            "    return self.gather(math_ops.range(0, self.size()), name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 323, in gather\n",
            "    element_shape=element_shape)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6705, in tensor_array_gather_v3\n",
            "    element_shape=element_shape, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "\n",
            "W0905 12:55:43.253796 140399066265472 register.py:64] Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.UnimplementedError: 2 root error(s) found.\n",
            "  (0) Unimplemented: TensorArray has size zero, but element shape [?] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\n",
            "\t [[{{node decoder_1/TensorArrayStack_1/TensorArrayGatherV3}}]]\n",
            "  (1) Unimplemented: TensorArray has size zero, but element shape [?,128] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\n",
            "\t [[{{node bidirectional_rnn_0/bw/bw/TensorArrayStack/TensorArrayGatherV3}}]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/itchat/components/register.py\", line 60, in configured_reply\n",
            "    r = replyFn(msg)\n",
            "  File \"<ipython-input-74-220cca16944d>\", line 5, in text_reply\n",
            "    reply_ = wechat_input(msg.text)\n",
            "  File \"<ipython-input-72-e63b7913d9db>\", line 7, in wechat_input\n",
            "    prediction = sess.run(model.predict_ids, feed_dict={model.X:batch_input,model.X_length:seq_input})\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.UnimplementedError: 2 root error(s) found.\n",
            "  (0) Unimplemented: TensorArray has size zero, but element shape [?] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\n",
            "\t [[node decoder_1/TensorArrayStack_1/TensorArrayGatherV3 (defined at <ipython-input-63-bff4a1c4fd7e>:119) ]]\n",
            "  (1) Unimplemented: TensorArray has size zero, but element shape [?,128] is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\n",
            "\t [[node bidirectional_rnn_0/bw/bw/TensorArrayStack/TensorArrayGatherV3 (defined at <ipython-input-63-bff4a1c4fd7e>:61) ]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "Original stack trace for 'decoder_1/TensorArrayStack_1/TensorArrayGatherV3':\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
            "    ioloop.IOLoop.instance().start()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n",
            "    handler_func(fd_obj, events)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n",
            "    self._handle_recv()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n",
            "    self._run_callback(callback, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n",
            "    callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
            "    return self.dispatch_shell(stream, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
            "    handler(stream, idents, msg)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
            "    user_expressions, allow_stdin)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
            "    interactivity=interactivity, compiler=compiler, result=result)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
            "    if self.run_code(code, result):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-67-4dbc035edcfb>\", line 5, in <module>\n",
            "    len(dictionary_ans), learning_rate, batch_size)\n",
            "  File \"<ipython-input-63-bff4a1c4fd7e>\", line 119, in __init__\n",
            "    maximum_iterations = 2 * tf.reduce_max(self.X_length))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in dynamic_decode\n",
            "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 515, in map_structure\n",
            "    structure[0], [func(*x) for x in entries],\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 515, in <listcomp>\n",
            "    structure[0], [func(*x) for x in entries],\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 461, in <lambda>\n",
            "    final_outputs = nest.map_structure(lambda ta: ta.stack(), final_outputs_ta)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 1205, in stack\n",
            "    return self._implementation.stack(name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 309, in stack\n",
            "    return self.gather(math_ops.range(0, self.size()), name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py\", line 323, in gather\n",
            "    element_shape=element_shape)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 6705, in tensor_array_gather_v3\n",
            "    element_shape=element_shape, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "\n",
            "LOG OUT!\n",
            "I0905 12:58:31.826570 140395767072512 login.py:283] LOG OUT!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWIwLBctDwX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}